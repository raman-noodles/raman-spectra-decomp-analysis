{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raman Spectroscopy Decomposition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Once components in a mixture Raman spectra have been identified and assigned, and psudo-Voigt curve fiting has been completed the next step is to compare pure component calibration (or non-decomposing, non-reacting) area under peaks to experimental data. From this comparision one will be able to deterimine:\n",
    "1. is decomposition occuring?\n",
    "\n",
    "and if it is then: \n",
    "2. calculate the amount of molar decomposition \n",
    "\n",
    "This calculation can be completed by comparing the area value of the experimental mixture Raman spectra to the pure component calibration Raman spectra area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step (1/2): Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initial imports\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import lineid_plot\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import dataimport\n",
    "from ramandecompy import datavis\n",
    "from ramandecompy import machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step (2/2): Import Calibration / Pure Component Raman Spectra Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx fit with compound pseudo-Voigt model. Results saved to calibration_data2.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx fit with compound pseudo-Voigt model. Results saved to calibration_data2.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/CO2_100wt%.csv fit with compound pseudo-Voigt model. Results saved to calibration_data2.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/water.xlsx fit with compound pseudo-Voigt model. Results saved to calibration_data2.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/sapphire.xlsx fit with compound pseudo-Voigt model. Results saved to calibration_data2.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/FormicAcid_3_6percent.xlsx fit with compound pseudo-Voigt model. Results saved to calibration_data2.hdf5.\n"
     ]
    }
   ],
   "source": [
    "dataprep.new_hdf5('calibration_data2')\n",
    "\n",
    "dataprep.add_calibration('calibration_data2.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                          label='Hydrogen')\n",
    "dataprep.add_calibration('calibration_data2.hdf5',\n",
    "                         '../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx',\n",
    "                         label='CarbonMonoxide')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/CO2_100wt%.csv',label='CO2')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/water.xlsx',label='H2O')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/sapphire.xlsx',label='sapphire')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/FormicAcid_3_6percent.xlsx',label='FormicAcid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** calibration_data2.hdf5 ****\n",
      "\u001b[1mCO2\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mCarbonMonoxide\u001b[0m\n",
      "|    Peak_01\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mFormicAcid\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    Peak_05\n",
      "|    Peak_06\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mH2O\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mHydrogen\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1msapphire\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n"
     ]
    }
   ],
   "source": [
    "dataprep.view_hdf5('calibration_data2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('calibration_data2-Copy1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** dataimport_ML_df-Copy1.hdf5 ****\n",
      "\u001b[1m300C\u001b[0m\n",
      "|    \u001b[1m25s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m35s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m45s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m55s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m65s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m320C\u001b[0m\n",
      "|    \u001b[1m25s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m30s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m40s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m50s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m60s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    Peak_20\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m340C\u001b[0m\n",
      "|    \u001b[1m20s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m30s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m40s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m50s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m60s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m360C\u001b[0m\n",
      "|    \u001b[1m20s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m30s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m40s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m50s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m60s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m380C\u001b[0m\n",
      "|    \u001b[1m15s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m25s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m35s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m45s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m55s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m390C\u001b[0m\n",
      "|    \u001b[1m10s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m15s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m20s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m25s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m30s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m400C\u001b[0m\n",
      "|    \u001b[1m10s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m125s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m15s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m5s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    Peak_19\n",
      "|    |    Peak_20\n",
      "|    |    Peak_21\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m75s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    Peak_18\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m410C\u001b[0m\n",
      "|    \u001b[1m10s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m125s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m15s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m5s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    Peak_15\n",
      "|    |    Peak_16\n",
      "|    |    Peak_17\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m75s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m420C\u001b[0m\n",
      "|    \u001b[1m10s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m5s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    Peak_14\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m625s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m75s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m875s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "\u001b[1m430C\u001b[0m\n",
      "|    \u001b[1m4s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m5s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m6s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m7s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n",
      "|    \u001b[1m8s\u001b[0m\n",
      "|    |    Peak_01\n",
      "|    |    Peak_02\n",
      "|    |    Peak_03\n",
      "|    |    Peak_04\n",
      "|    |    Peak_05\n",
      "|    |    Peak_06\n",
      "|    |    Peak_07\n",
      "|    |    Peak_08\n",
      "|    |    Peak_09\n",
      "|    |    Peak_10\n",
      "|    |    Peak_11\n",
      "|    |    Peak_12\n",
      "|    |    Peak_13\n",
      "|    |    counts\n",
      "|    |    residuals\n",
      "|    |    wavenumber\n"
     ]
    }
   ],
   "source": [
    "dataprep.view_hdf5('dataimport_ML_df-Copy1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Experimental Data Sets\n",
    "The first thing is to put experimental data into a hdf5 file (this file will end up being used to identify peaks)\n",
    "\n",
    "With multiple files in a directory/ many data sets it is usefull to loop over all files in the directory to add versus adding one by one. The code to loop came from a stackoverflow comment: `https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory`\n",
    "\n",
    "Note: A good resource for HDF5 file types in general is: `http://docs.h5py.org/en/stable/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataprep.new_hdf5('dataprep_experiment') #comment this line out once made for the first time so an error isn't given saying that the file already exists\n",
    "#directory = '/Users/elizabeth/Desktop/raman-spectra-decomp-analysis/ramandecompy/tests/test_files/' #defining directory for data\n",
    "#dataprep.view_hdf5('dataprep_experimental.hdf5')\n",
    "\n",
    "#base_dir = '../ramandecompy/tests/test_files/'\n",
    "\n",
    "#for filename in os.listdir(directory):\n",
    "#    if filename.startswith('FA_') and filename.endswith('.csv'):\n",
    "#        locationandfile = directory + filename\n",
    "#        dataprep.add_experiment('dataprep_experimental.hdf5', locationandfile)\n",
    "#        continue\n",
    "#    else:\n",
    "#        continue\n",
    "#return\n",
    "\n",
    "#FOR CALIBRATION DATA MASS ADD\n",
    "\n",
    "#dataprep.new_hdf5('dataprep_experiment') #comment this line out once made for the first time so an error isn't given saying that the file already exists\n",
    "#directory = '/Users/elizabeth/Desktop/raman-spectra-decomp-analysis/ramandecompy/tests/test_files/' #defining directory for data\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#type(filename) #checking the type (making sure is a string) for file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dataprep.view_hdf5('dataimport_ML_df-Copy1.hdf5') #making sure the loop did its job and all data is correctly imported \n",
    "#comment out this to not see the long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define substance of interest\n",
    "The second step is to determine if the desired speciecies in the spectra is present, and if it is then if it has decomposed (decreased/changed) from the defined calibration area. \n",
    "\n",
    "At this point this will be done by the user knowing where the approximate location of the peak for the substance that is of interest. \n",
    "\n",
    "Given the user center peak wavelength location input the code will go through the calibration data and for a peak with a center at the defined location (ith some tolerance of +/- 10 cm^-1) will take the area of that curve and store it as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting peak information for Formic Acid \n",
    "\n",
    "data1 = h5py.File('calibration_data2.hdf5', 'r+')\n",
    "# then specify the peak\n",
    "peak_01 = list(data1['FormicAcid/Peak_01'][0])\n",
    "#peak_01s = data1['FormicAcid/Peak_01']\n",
    "# you put list because otherwise it just saves it as a h5py.dataset or something and lists are more familiar. Then peak_01 will be a list containing the 7 elements of the Peak_01 dataset\n",
    "print(peak_01)\n",
    "\n",
    "type(peak_01)\n",
    "#print(type(peak_01s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_01[0] #Looking for area under the curve value for the first peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_01[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out what peak from Formic Acid to use/ are close to other observed peaks used in analysis\n",
    "For Formic Acid, prior reports of the wavenumbers of significant Raman Peaks (cm^-1) were at:\n",
    "- 712\n",
    "- 1219\n",
    "- 1400\n",
    "- 1714\n",
    "- 2943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak_01 = list(data1['FormicAcid/Peak_01'])\n",
    "#print(peak_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak_02 = list(data1['FormicAcid/Peak_02'])\n",
    "#print(peak_02[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak_03 = list(data1['FormicAcid/Peak_03'])\n",
    "#print(peak_03[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak_04 = list(data1['FormicAcid/Peak_04'])\n",
    "#print(peak_04[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak_05 = list(data1['FormicAcid/Peak_05'])\n",
    "#print(peak_05[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak_06 = list(data1['FormicAcid/Peak_06'])\n",
    "#print(peak_06[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All peaks previously reported are identified in the calibration file within +/- 5 wavenumbers\n",
    "There is an additional peak identified at 1055, but it is hypothesized that this peak may not be easily identified from other components with similar wavenumbers and/or with the amplitude of the peak being smaller then peaks of formic acid at the other wavenumbers this could be why it isn't identifed in literature. \n",
    "\n",
    "For this example the peak occuring at 1400 cm^-1 (peak_04) will be used for molar concentration calculations **because... NEED TO FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FA_cal_area = peak_04[6]\n",
    "#print(FA_cal_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define presence of substance in experimental data\n",
    "\n",
    "Then for that same center peak wavelength location input it will identify the presence of the peak (if it is there) in the experimental data and area for that peak and store it as a second variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyfinder function\n",
    "def keyfinder(hdf5_filename):\n",
    "   seconds = []\n",
    "   hdf5 = h5py.File(hdf5_filename, 'r')\n",
    "   for _, layer_1 in enumerate(list(hdf5.keys())):\n",
    "       if isinstance(hdf5[layer_1], h5py.Group):\n",
    "   #         print('\\033[1m{}\\033[0m'.format(layer_1))\n",
    "           for _, layer_2 in enumerate(list(hdf5[layer_1].keys())):\n",
    "               if isinstance(hdf5['{}/{}'.format(layer_1, layer_2)], h5py.Group):\n",
    "   #                 print('|    \\033[1m{}\\033[0m'.format(layer_2))\n",
    "                   seconds.append('{}/{}'.format(layer_1, layer_2))\n",
    "                   for _, layer_3 in enumerate(list(hdf5['{}/{}'.format(layer_1, layer_2)])):\n",
    "                       if isinstance(hdf5['{}/{}/{}'.format(layer_1, layer_2, layer_3)],\n",
    "                                     h5py.Group):\n",
    "   #                         print('|    |    \\033[1m{}\\033[0m/...'.format(layer_3))\n",
    "                           pass\n",
    "                       else:\n",
    "                           pass\n",
    "   #                         print('|    |    {}'.format(layer_3))\n",
    "               else:\n",
    "   #                 print('|    {}'.format(layer_2))\n",
    "                   seconds.append('{}/{}'.format(layer_1, layer_2))\n",
    "       else:\n",
    "           pass\n",
    "   #         print('{}'.format(layer_1))\n",
    "   hdf5.close()\n",
    "   return seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define filenames\n",
    "hdf5_calfilename = 'calibration_data2.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy1.hdf5'\n",
    "\n",
    "cal_key_list = machine_learning.keyfinder(hdf5_calfilename)\n",
    "exp_key_list = machine_learning.keyfinder(hdf5_expfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CO2/Peak_01', 'CO2/Peak_02', 'CO2/counts', 'CO2/residuals', 'CO2/wavenumber', 'CarbonMonoxide/Peak_01', 'CarbonMonoxide/counts', 'CarbonMonoxide/residuals', 'CarbonMonoxide/wavenumber', 'FormicAcid/Peak_01', 'FormicAcid/Peak_02', 'FormicAcid/Peak_03', 'FormicAcid/Peak_04', 'FormicAcid/Peak_05', 'FormicAcid/Peak_06', 'FormicAcid/counts', 'FormicAcid/residuals', 'FormicAcid/wavenumber', 'H2O/Peak_01', 'H2O/Peak_02', 'H2O/counts', 'H2O/residuals', 'H2O/wavenumber', 'Hydrogen/Peak_01', 'Hydrogen/Peak_02', 'Hydrogen/Peak_03', 'Hydrogen/Peak_04', 'Hydrogen/counts', 'Hydrogen/residuals', 'Hydrogen/wavenumber', 'sapphire/Peak_01', 'sapphire/Peak_02', 'sapphire/Peak_03', 'sapphire/Peak_04', 'sapphire/counts', 'sapphire/residuals', 'sapphire/wavenumber']\n"
     ]
    }
   ],
   "source": [
    "print(cal_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['300C/25s', '300C/35s', '300C/45s', '300C/55s', '300C/65s', '320C/25s', '320C/30s', '320C/40s', '320C/50s', '320C/60s', '340C/20s', '340C/30s', '340C/40s', '340C/50s', '340C/60s', '360C/20s', '360C/30s', '360C/40s', '360C/50s', '360C/60s', '380C/15s', '380C/25s', '380C/35s', '380C/45s', '380C/55s', '390C/10s', '390C/15s', '390C/20s', '390C/25s', '390C/30s', '400C/10s', '400C/125s', '400C/15s', '400C/5s', '400C/75s', '410C/10s', '410C/125s', '410C/15s', '410C/5s', '410C/75s', '420C/10s', '420C/5s', '420C/625s', '420C/75s', '420C/875s', '430C/4s', '430C/5s', '430C/6s', '430C/7s', '430C/8s']\n"
     ]
    }
   ],
   "source": [
    "print(exp_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300C/25s',\n",
       " '300C/35s',\n",
       " '300C/45s',\n",
       " '300C/55s',\n",
       " '300C/65s',\n",
       " '320C/25s',\n",
       " '320C/30s',\n",
       " '320C/40s',\n",
       " '320C/50s',\n",
       " '320C/60s',\n",
       " '340C/20s',\n",
       " '340C/30s',\n",
       " '340C/40s',\n",
       " '340C/50s',\n",
       " '340C/60s',\n",
       " '360C/20s',\n",
       " '360C/30s',\n",
       " '360C/40s',\n",
       " '360C/50s',\n",
       " '360C/60s',\n",
       " '380C/15s',\n",
       " '380C/25s',\n",
       " '380C/35s',\n",
       " '380C/45s',\n",
       " '380C/55s',\n",
       " '390C/10s',\n",
       " '390C/15s',\n",
       " '390C/20s',\n",
       " '390C/25s',\n",
       " '390C/30s',\n",
       " '400C/10s',\n",
       " '400C/125s',\n",
       " '400C/15s',\n",
       " '400C/5s',\n",
       " '400C/75s',\n",
       " '410C/10s',\n",
       " '410C/125s',\n",
       " '410C/15s',\n",
       " '410C/5s',\n",
       " '410C/75s',\n",
       " '420C/10s',\n",
       " '420C/5s',\n",
       " '420C/625s',\n",
       " '420C/75s',\n",
       " '420C/875s',\n",
       " '430C/4s',\n",
       " '430C/5s',\n",
       " '430C/6s',\n",
       " '430C/7s',\n",
       " '430C/8s']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list= machine_learning.keyfinder('dataimport_ML_df-Copy1.hdf5')\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes in compounds from a dictionary from shoyu, and, using spectrafit,\n",
    "identifies peaks found in both the fed-in known spectra, as well as the unknown spectra\n",
    "to be analyzed. From that identification, it then classifies the peaks in the unknown\n",
    "spectra based on the fed-in known spectra.\n",
    " \"\"\"\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lineid_plot\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Will probably need to create an additional function\n",
    "\n",
    "def peak_assignment(unknownhdf5_filename, key, knownhdf5_filename,\n",
    "                    precision=10, exportlabelinput=True, plot=True):\n",
    "    \"\"\"This function is a wrapper function from which all classification\n",
    "    of peaks occurs.\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(unknownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `unknownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(unknownhdf5_filename)))\n",
    "    if not unknownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`unknownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + unknownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(key, str):\n",
    "        raise TypeError(\"\"\"Passed value of `key` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(key)))\n",
    "    if not isinstance(precision, (float, int)):\n",
    "        raise TypeError(\"\"\"Passed value of `precision` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(precision)))\n",
    "    if not isinstance(exportlabelinput, bool):\n",
    "        raise TypeError(\"\"\"Passed value of `exportlabelinput` is not a Boolean!\n",
    "        Instead, it is: \"\"\" + str(type(exportlabelinput)))\n",
    "    if not isinstance(plot, bool):\n",
    "        raise TypeError(\"\"\"Passed value of `plot` is not a Boolean!\n",
    "        Instead, it is: \"\"\" + str(type(plot)))\n",
    "    # open .hdf5\n",
    "    unhdf5 = h5py.File(unknownhdf5_filename, 'r+')\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r+')\n",
    "\n",
    "    #Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_compound_list)))\n",
    "    #Now we need to check the elements within the known_compound_list\n",
    "    #to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list` is not\n",
    "            a string! Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "\n",
    "    # extract spectra data\n",
    "    unknown_x = list(unhdf5['{}/wavenumber'.format(key)])\n",
    "    unknown_y = list(unhdf5['{}/counts'.format(key)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    #Lets identify the peaks in the unknown spectrum.\n",
    "    unknown_peaks = []\n",
    "    for i, peak in enumerate(list(unhdf5['{}'.format(key)])[:-3]):\n",
    "        try:\n",
    "            if i < 9:\n",
    "                unknown_peaks.append(list(unhdf5['{}/Peak_0{}'.format(key,\n",
    "                                                                      i+1)])[0][2])\n",
    "            else:\n",
    "                unknown_peaks.append(list(unhdf5['{}/Peak_{}'.format(key,\n",
    "                                                                     i+1)])[0][2])\n",
    "        except Exception as e:\n",
    "            #Normal peakassignment\n",
    "            print(\"\"\"Function did not receive normal peak.\n",
    "            The function continued to look for an adjusted peak.\"\"\")\n",
    "            if i < 9:\n",
    "                print(peak)\n",
    "                unknown_peaks.append(list(unhdf5['{}/Peak_0{}*'.format(key,\n",
    "                                                                       i+1)])[0][2])\n",
    "            else:\n",
    "                unknown_peaks.append(list(unhdf5['{}/Peak_{}*'.format(key,\n",
    "                                                                      i+1)])[0][2])\n",
    "            print('Peak_{}*'.format(i+1))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    #OK, next identify all of the peaks present in the known compound set.\n",
    "    #For efficiency, we'll also compare them against the unknown in the same for loop.\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    num_peaks_list = []\n",
    "    assignment_matrix = []\n",
    "    split__index_list = []\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        print(\"The peaks that we found for \"\n",
    "              + str(known_compound_list[i]) + \" are: \")\n",
    "        num_peaks_list.append(len(list(knhdf5[known_compound_list[i]])[:-3]))\n",
    "        split__index_list.append(sum(num_peaks_list))\n",
    "        for j, peak in enumerate(list(knhdf5[known_compound_list[i]])[:-3]):\n",
    "            print(list(knhdf5['{}/{}'.format(known_compound_list[i], peak)])[0][2])\n",
    "            # Need to separate known peaks to make a list of two separate lists\n",
    "            # to perform custom list split using list comprehension + zip() and split_index_list\n",
    "            known_peaks_list.append(list(knhdf5['{}/{}'.format(known_compound_list[i],\n",
    "                                                               peak)])[0][2])\n",
    "            result = [known_peaks_list[i : j] for i, j in zip([0] + split__index_list,\n",
    "                                                              split__index_list +\n",
    "                                                              [None])]\n",
    "        known_peaks.append(result)\n",
    "        assignment_matrix.append(compare_unknown_to_known(\n",
    "            unknown_peaks, known_peaks[i][i], precision))\n",
    "    #Ok, so that generates a full association matrix that contains everything\n",
    "    #we need to assign peaks.\n",
    "    #Now, let's go through and actually assign text to peaks.\n",
    "    unknown_peak_assignments = peak_position_comparisons(unknown_peaks,\n",
    "                                                         known_peaks,\n",
    "                                                         assignment_matrix,\n",
    "                                                         knownhdf5_filename)\n",
    "    print(unknown_peak_assignments)\n",
    "    peak_labels = []\n",
    "    for i, _ in enumerate(unknown_peak_assignments):\n",
    "        peak_labels.append(str(unknown_peak_assignments[i]))\n",
    "    frames = []\n",
    "    for j, peak in enumerate(list(unhdf5['{}'.format(key)])[:-3]):\n",
    "        dat = add_label(unknownhdf5_filename, key, peak, peak_labels[j])\n",
    "        frames.append(dat)\n",
    "            \n",
    "         \n",
    "    df = pd.concat(frames,axis=1, join='outer', join_axes=None, ignore_index=False,\n",
    "              keys=None, levels=None, names=None, verify_integrity=False,\n",
    "              copy=True,sort=True)\n",
    "    df =df.T\n",
    "    if plot:\n",
    "        plotting_peak_assignments(unknown_x,\n",
    "                                  unknown_y,\n",
    "                                  unknown_peaks,\n",
    "                                  unknown_peak_assignments,\n",
    "                                  unknownhdf5_filename,\n",
    "                                  knownhdf5_filename,\n",
    "                                  key,\n",
    "                                  peak_labels,\n",
    "                                  exportlabelinput)\n",
    "\n",
    "    percentages = percentage_of_peaks_found(known_peaks[len(known_compound_list)-1],\n",
    "                                            assignment_matrix,\n",
    "                                            knownhdf5_filename)\n",
    "    print(percentages)\n",
    "    knhdf5.close()\n",
    "    unhdf5.close()\n",
    "    return df\n",
    "\n",
    "def compare_unknown_to_known(unknown_peaks, known_peaks, precision):\n",
    "    \"\"\"This function takes in peak positions for the spectrum to be\n",
    "    analyzed and a single known compound and determines if the peaks\n",
    "    found in the known compound are present in the unknown spectrum.\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(unknown_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `combined_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_peaks)))\n",
    "\n",
    "    if not isinstance(known_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_peaks)))\n",
    "# Need to check values within known_peaks\n",
    "\n",
    "    if not isinstance(precision, (float, int)):\n",
    "        raise TypeError(\"\"\"Passed value of `precision` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(precision)))\n",
    "\n",
    "    assignment_matrix = np.zeros(len(unknown_peaks))\n",
    "    peaks_found = 0\n",
    "    for i, _ in enumerate(unknown_peaks):\n",
    "        for j, _ in enumerate(known_peaks):\n",
    "            # instead of If, call peak_1D_score\n",
    "            if math.isclose(unknown_peaks[i], known_peaks[j],\n",
    "                            abs_tol=precision, rel_tol=1e-9):\n",
    "                # Instead of using a 1, just input the score\n",
    "                # from the score calculator.\n",
    "                # Bigger is better.\n",
    "                # Storing only the second component in the list.\n",
    "                assignment_matrix[i] = 1\n",
    "                peaks_found += 1\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "        if peaks_found == len(known_peaks):\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "    print(assignment_matrix)\n",
    "\n",
    "    return assignment_matrix\n",
    "\n",
    "def peak_position_comparisons(unknown_peaks, known_compound_peaks,\n",
    "                              association_matrix,\n",
    "                              knownhdf5_filename):\n",
    "    \"\"\"This function takes in an association matrix and turns the numbers\n",
    "    given by said matrix into a text label.\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(unknown_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `unknown_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_peaks)))\n",
    "    if not isinstance(known_compound_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_compound_peaks)))\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\"+ str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\"+ knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(association_matrix, list):\n",
    "        raise TypeError(\"\"\"Passed value of `association_matrix` is not a float\n",
    "        or int! Instead, it is: \"\"\" + str(type(association_matrix)))\n",
    "\n",
    "    # open .hdf5\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r+')\n",
    "\n",
    "    #Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\"+ str(type(known_compound_list)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list` is\n",
    "            not a string! Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "\n",
    "    unknown_peak_assignment = []\n",
    "    #Step through the unknown peaks to make an assignment for each unknown peak.\n",
    "\n",
    "    for i, _ in enumerate(unknown_peaks):\n",
    "        # We might be able to make a small performance\n",
    "        # improvement if we were to somehow\n",
    "        # not search the peaks we already had searched\n",
    "        # but that seems to not be trivial.\n",
    "        position_assignment = []\n",
    "        # We'll need an outer loop that walks through\n",
    "        # all the different compound positions\n",
    "        for j, _ in enumerate(known_compound_peaks):\n",
    "            if association_matrix[j][i] == 1:\n",
    "                position_assignment.append(known_compound_list[j])\n",
    "            else:\n",
    "                pass\n",
    "        if position_assignment == []:\n",
    "            position_assignment.append(\"Unassigned\")\n",
    "        else:\n",
    "            pass\n",
    "        unknown_peak_assignment.append(position_assignment)\n",
    "    knhdf5.close()\n",
    "    return unknown_peak_assignment\n",
    "\n",
    "\n",
    "def percentage_of_peaks_found(known_peaks, association_matrix, knownhdf5_filename):\n",
    "    \"\"\"This function takes in a list of classified peaks, and returns a percentage of\n",
    "    how many of the material's peaks are found in the unknown spectrum.\n",
    "    This can be used as a metric of confidence.\"\"\"\n",
    "\n",
    "    #Handle bad inputs\n",
    "    if not isinstance(known_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_peaks)))\n",
    "    if not isinstance(association_matrix, list):\n",
    "        raise TypeError(\"\"\"Passed value of `association_matrix` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(association_matrix)))\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\"+ str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "\n",
    "    # open .hdf5\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r+')\n",
    "\n",
    "    # Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\"+ str(type(known_compound_list)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list`\n",
    "            is not a string!\n",
    "            Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "    percentage_dict = {}\n",
    "    for j, _ in enumerate(known_compound_list):\n",
    "#         print(association_matrix)\n",
    "#         print(known_peaks)\n",
    "        count_number = sum(association_matrix[j])\n",
    "        percentage_dict[known_compound_list[j]] = float(count_number /\n",
    "                                                        (len(known_peaks[j]))) * 100\n",
    "    knhdf5.close()\n",
    "    return percentage_dict\n",
    "\n",
    "\n",
    "def plotting_peak_assignments(unknown_x, unknown_y, unknown_peaks,\n",
    "                              unknown_peak_assignments, unknownhdf5_filename,\n",
    "                              knownhdf5_filename, key, peak_labels,\n",
    "                              exportlabelinput=True):\n",
    "    \"\"\"This function plots a set of unknown peaks, and plots the assigned\n",
    "    classification given by the functions within peakassignment\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(unknown_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `unknown_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_peaks)))\n",
    "\n",
    "    if not isinstance(unknown_x, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `unknown_x` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_x)))\n",
    "\n",
    "    if not isinstance(unknown_y, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\" Passed value of `unknown_y` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_y)))\n",
    "    # handling input errors\n",
    "    if not isinstance(unknownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `unknownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(unknownhdf5_filename)))\n",
    "    if not unknownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`unknownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + unknownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(key, str):\n",
    "        raise TypeError(\"\"\"Passed value of `key` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(key)))\n",
    "    if not isinstance(peak_labels, list):\n",
    "        raise TypeError(\"\"\"Passed value of `peak_labels` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(peak_labels)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(peak_labels):\n",
    "        if not isinstance(peak_labels[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `peak_labels` is not a string!\n",
    "            Instead, it is: \"\"\" + str(type(peak_labels[i])))\n",
    "    #Now we need to check the elements within the unknown_peak_assignment\n",
    "    #to make sure they are correct.\n",
    "    for i, _ in enumerate(unknown_peak_assignments):\n",
    "        if not isinstance(unknown_peak_assignments[i], list):\n",
    "            raise TypeError(\"\"\"Passed value within `unknown_peak_assignment`\n",
    "            is not a list!\n",
    "            Instead, it is: \"\"\" + str(type(unknown_peak_assignments[i])))\n",
    "            if not isinstance(unknown_peak_assignments[i][i], str):\n",
    "                raise TypeError(\"\"\"Passed value within `unknown_peak_assignment`\n",
    "                is not a string! \n",
    "                Instead, it is: \"\"\" + str(type(unknown_peak_assignments[i][i])))\n",
    "    # open .hdf5\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r')\n",
    "    unhdf5 = h5py.File(unknownhdf5_filename, 'r')\n",
    "    residuals = np.asarray(list(unhdf5['{}/residuals'.format(key)]))\n",
    "    #Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_compound_list)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list` is\n",
    "            not a string! Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "    # extract spectra data\n",
    "    x_data = list(unhdf5['{}/wavenumber'.format(key)])\n",
    "    y_data = list(unhdf5['{}/counts'.format(key)])\n",
    "#     plt.plot(unknown_x, unknown_y, color='black', label='Unknown Spectrum')\n",
    "    if exportlabelinput:\n",
    "        print('export labelling only')\n",
    "    else:\n",
    "        peak_labels = []\n",
    "        for i, _ in enumerate(unknown_peak_assignments):\n",
    "            peak_labels.append(str(unknown_peak_assignments[i]))\n",
    "    print(peak_labels)\n",
    "    # plot spectra and peak labels\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True,\n",
    "                                   gridspec_kw={'height_ratios': [3, 1]},\n",
    "                                   figsize=(15, 6), dpi=300)\n",
    "    # plot data\n",
    "    ax1.plot(x_data, y_data, color='blue')\n",
    "    ax2.plot(x_data, residuals, color='teal')\n",
    "    lineid_plot.plot_line_ids(x_data, y_data, unknown_peaks,\n",
    "                              peak_labels, box_axes_space=0.30,\n",
    "                              plot_kwargs={'linewidth':1},\n",
    "                              max_iter=75, ax=ax1)\n",
    "#     fig.set_size_inches(15,5)\n",
    "    # lock the scale so that additional plots do not warp the labels\n",
    "    ax1.set_autoscale_on(False)\n",
    "    # Titles and labels\n",
    "    ax2.set_xlabel('Wavenumber ($cm^{-1}$)', fontsize=14)\n",
    "    ax1.set_xlim(min(x_data), max(x_data))\n",
    "    ax1.set_ylabel('Counts', fontsize=14, labelpad=20)\n",
    "    ax2.set_ylabel('Residuals', fontsize=14, labelpad=12)\n",
    "    # scale residuals plot symmetrically about zero\n",
    "    ylim = max(abs(min(residuals)), abs(max(residuals)))\n",
    "    ax2.set_ylim(-ylim, ylim)\n",
    "    # add grid lines to residual plot\n",
    "    ax2.grid(which='major', axis='y', linestyle='-')\n",
    "    # force tick labels for top plot\n",
    "    ax1.tick_params(axis='both', which='both', labelsize=10, labelbottom=True)\n",
    "    # add title\n",
    "    ax1.set_title('{} spectra from {}'.format(key, unknownhdf5_filename),\n",
    "                  fontsize=18, pad=350)\n",
    "    plt.show()\n",
    "    knhdf5.close()\n",
    "    unhdf5.close()\n",
    "\n",
    "def add_label(hdf5_filename, key, peak, label):\n",
    "    \"\"\"Function that adds a label to a peak dataset in the hdf5 file\n",
    "    \"\"\"\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(hdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `hdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(hdf5_filename)))\n",
    "    if not hdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`hdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + hdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(key, str):\n",
    "        raise TypeError(\"\"\"Passed value of `key` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(key)))\n",
    "    if not isinstance(key, str):\n",
    "        raise TypeError(\"\"\"Passed value of `key` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(key)))\n",
    "    if not isinstance(peak, str):\n",
    "        raise TypeError(\"\"\"Passed value of `peak` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(peak)))\n",
    "    if not isinstance(label, str):\n",
    "        raise TypeError(\"\"\"Passed value of `label` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(label)))\n",
    "    # open hdf5 file as read/write\n",
    "    hdf5 = h5py.File(hdf5_filename, 'r+')\n",
    "    # extract existing data from peak dataset\n",
    "    peak_data = list(hdf5['{}/{}'.format(key, peak)])[0]\n",
    "#     print(peak_data)\n",
    "    # make a new tuple that contains the orginal data as well as the label\n",
    "    label_tuple = (label,)\n",
    "    data = tuple(peak_data) +label_tuple\n",
    "    # delete the old dataset so the new one can be saved\n",
    "    del hdf5['{}/{}'.format(key, peak)]\n",
    "    # define a custom datatype that allows for a string as the the last tuple element\n",
    "    my_datatype = np.dtype([('fraction', np.float),\n",
    "                            ('center', np.float),\n",
    "                            ('sigma', np.float),\n",
    "                            ('amplitude', np.float),\n",
    "                            ('fwhm', np.float),\n",
    "                            ('height', np.float),\n",
    "                            ('area under the curve', np.float),\n",
    "                            ('label', h5py.special_dtype(vlen=str))])\n",
    "    # recreate the old dataset in the hdf5 file\n",
    "    dataset = hdf5.create_dataset('{}/{}'.format(key, peak),\n",
    "                                  (1,), dtype=my_datatype)\n",
    "    # apply custom dtype to data tuple\n",
    "#     print(dataset)\n",
    "    print(data)\n",
    "#     print(my_datatype)\n",
    "    data_array = np.array(data, dtype=my_datatype)\n",
    "    # write new values to the blank dataset\n",
    "    dataset[...] = data_array\n",
    "#     print(dataset)\n",
    "    hdf5.close()\n",
    "    return data_array \n",
    "\n",
    "def peak_1d_score(row_i, row_j, scoremax, precision):\n",
    "    \"\"\"\n",
    "    Returns scores with respect to the repricoal of the\n",
    "    calculated Euclidean distance between peaks\n",
    "    #((x1-x2)^2) in 1D\n",
    "    #((x1-x2)^2 + (y1-y2)^2) in 2D\n",
    "\n",
    "    Parameters:\n",
    "        row_i (list like):  input list\n",
    "        row_j (list like): input list\n",
    "        scoremax (float): Euclidean reciprocal score divided by max score;\n",
    "        default is 1\n",
    "\n",
    "    Returns:\n",
    "        scores (list): Euclidean reciprocal scores\n",
    "        peaks (tuple): peaks associated with scores\n",
    "    \"\"\"\n",
    "    # Handling errors at the input\n",
    "    if not isinstance(row_i, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_i` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_i)))\n",
    "    if not isinstance(row_j, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_j` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_j)))\n",
    "    if not isinstance(scoremax, (float, int)):\n",
    "        raise TypeError(\"\"\"Passed value of `scoremax` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(scoremax)))\n",
    "    if scoremax < 0:\n",
    "        raise ValueError(\"\"\"Passed value of `scoremax` is not within bounds!\"\"\")\n",
    "\n",
    "    # Initializing the variables\n",
    "    scores = []\n",
    "    peaks = []\n",
    "\n",
    "    for i, _ in enumerate(row_i):\n",
    "        for j, _ in enumerate(row_j):\n",
    "            # Calculating distances between peaks\n",
    "            distance = np.where((row_i[i] - row_j[j] > precision), np.nan,\n",
    "                                math.sqrt(sum([math.pow(row_i[i] - row_j[j], 2)])))\n",
    "            # Score for peaks less than 50 units apart\n",
    "            if 1 / (distance + 1) > (1/precision):\n",
    "                # Dividing over the given max score\n",
    "                scores.append(((1 / (distance + 1)) / scoremax))\n",
    "                # Appends a tuple of the compared peaks\n",
    "                peaks.append((row_i[i], row_j[j]))\n",
    "            else:\n",
    "                pass\n",
    "    return scores, peaks\n",
    "\n",
    "\n",
    "def score_max(row_i, row_j, k, precision):\n",
    "    \"\"\"\n",
    "    Returns list of scores sorted with respect to the peaks\n",
    "    related to its output max score\n",
    "\n",
    "    Parameters:\n",
    "        row_i (list like):  input list\n",
    "        row_j (list like): input list\n",
    "        k (int): input integer used to sort the scores / kth highest score\n",
    "\n",
    "    Returns:\n",
    "        maxscores (list): Euclidean reciprocal score divided by max score\n",
    "        maxpeaks (tuple): peaks associated with max scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Handling errors at the input\n",
    "    if not isinstance(row_i, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_i` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_i)))\n",
    "    if not isinstance(row_j, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_j` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_j)))\n",
    "    if not isinstance(k, int):\n",
    "        raise TypeError(\"\"\"Passed value of `k` is not an int!\n",
    "        Instead, it is: \"\"\" + str(type(k)))\n",
    "    if k < 0:\n",
    "        raise ValueError(\"\"\"Passed value of `k` is not within bounds!\"\"\")\n",
    "    try:\n",
    "        scoremax = sorted(set(peak_1d_score(row_i, row_j, 1, precision)[0][:]))[-k]\n",
    "        maxscores, maxpeaks = peak_1d_score(row_i, row_j, scoremax, precision)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\"\"Function did not receive a scoremax variable. The variable\n",
    "        scoremax has been reset back to 1. This is equivalent to \n",
    "        your unnormalized score.\"\"\")\n",
    "\n",
    "        maxscores, maxpeaks = peak_1d_score(row_i, row_j, 1, precision)\n",
    "\n",
    "    return maxscores, maxpeaks\n",
    "\n",
    "\n",
    "def score_sort(row_i, row_j, k, precision):\n",
    "    \"\"\"\n",
    "    Returns list of scores sorted\n",
    "\n",
    "    Parameters:\n",
    "        list_input (list like):  input list\n",
    "        row (list like): input list\n",
    "        k (int): input integer used to sort the scores / kth highest score\n",
    "\n",
    "    Returns:\n",
    "        sortedscores (list): sorted Euclidean distances\n",
    "    \"\"\"\n",
    "    # Handling errors at the input\n",
    "    if not isinstance(row_i, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_i` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_i)))\n",
    "    if not isinstance(row_j, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_j` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_j)))\n",
    "    if not isinstance(k, int):\n",
    "        raise TypeError(\"\"\"Passed value of `k` is not an int!\n",
    "        Instead, it is: \"\"\" + str(type(k)))\n",
    "    if k < 0:\n",
    "        raise ValueError(\"\"\"Passed value of `k` is not within bounds!\"\"\")\n",
    "\n",
    "    sortedscores = []\n",
    "    sortedscores.append(score_max(row_i, row_j, k, precision))\n",
    "    sortedscores.sort()\n",
    "\n",
    "    return sortedscores\n",
    "def process_score(unknown_peaks,known_peaks,k, precision, unknownname, knownname):\n",
    "    \"documentation\"\n",
    "    if k<len(known_peaks)+1:\n",
    "        compdf=pd.DataFrame(data=peakidentify.score_sort(unknown_peaks,known_peaks,k, precision)[0][0][:],\n",
    "                            columns=[str(unknownname)+'_vs_'+str(knownname)+'_peak_Scores normalized over the #'+\n",
    "                                     str(k) + ' highest score in the peak set'])\n",
    "        compdf=compdf.assign(Peaks=peakidentify.score_sort(unknown_peaks,known_peaks,1, precision)[0][1][:])\n",
    "    else:\n",
    "        compdf=pd.DataFrame(data=peakidentify.score_sort(unknown_peaks,known_peaks,k, precision)[0][0][:],\n",
    "                            columns=[str(unknownname)+'_vs_'+str(knownname)+'_peak_Scores Unnormalized'])\n",
    "        compdf=compdf.assign(Peaks=peakidentify.score_sort(unknown_peaks,known_peaks,1, precision)[0][1][:])\n",
    "    return compdf\n",
    "def score_table(unknown_peaks,known_peaks, precision,unknownname,knownname):\n",
    "    \"documentation\"\n",
    "    k_range = range(1,len(known_peaks)+2)\n",
    "    frames = [ process_score(unknown_peaks,known_peaks,k, precision,unknownname,knownname) for k in k_range ]\n",
    "    result = pd.concat(frames,axis=1, join='outer', join_axes=None, ignore_index=False,\n",
    "              keys=None, levels=None, names=None, verify_integrity=False,\n",
    "              copy=True,sort=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peaks that we found for CO2 are: \n",
      "1280.4\n",
      "1385.3\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "The peaks that we found for CarbonMonoxide are: \n",
      "2139.9096496496495\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "The peaks that we found for FormicAcid are: \n",
      "707.31\n",
      "1055.9\n",
      "1219.5\n",
      "1400.1\n",
      "1716.7\n",
      "2940.6\n",
      "[0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "The peaks that we found for H2O are: \n",
      "1640.6\n",
      "3194.4\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "The peaks that we found for Hydrogen are: \n",
      "355.6504104104104\n",
      "587.3333133133133\n",
      "816.0073473473473\n",
      "1035.6547747747748\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "The peaks that we found for sapphire are: \n",
      "378.71\n",
      "418.14\n",
      "575.97\n",
      "751.21\n",
      "[0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[['Unassigned'], ['sapphire'], ['sapphire'], ['Unassigned'], ['Hydrogen', 'sapphire'], ['FormicAcid'], ['sapphire'], ['FormicAcid'], ['FormicAcid'], ['FormicAcid'], ['H2O'], ['FormicAcid'], ['CarbonMonoxide'], ['Unassigned'], ['FormicAcid'], ['H2O']]\n",
      "(2.666322015842937e-06, 9.477841225773703, 314.77, 251.85262399390513, 18.955682451547407, 12.48172103404355, 251.85249801424473, \"['Unassigned']\")\n",
      "(0.5068130496276333, 4.012901918496308, 378.71, 1034.0055449667643, 8.025803836992615, 101.25996269002704, 1028.5886858163933, \"['sapphire']\")\n",
      "(0.6367815519010733, 3.8475936151059624, 418.14, 7306.162432573391, 7.695187230211925, 708.8643986283209, 7270.348693191736, \"['sapphire']\")\n",
      "(2.5523146723549495e-05, 2.6996216949362064, 448.67, 193.73416595859723, 5.399243389872413, 33.70835266609269, 193.74061611467974, \"['Unassigned']\")\n",
      "(0.18866601766321645, 4.210622440160284, 578.11, 1090.4680008060532, 8.421244880320568, 114.2498966320069, 1089.4878695237358, \"['Hydrogen', 'sapphire']\")\n",
      "(1.3877787807814457e-15, 20.49787213344091, 711.51, 6837.1469563207365, 40.99574426688182, 156.67652449243113, 6837.146565096084, \"['FormicAcid']\")\n",
      "(6.887132072419533e-07, 7.194622072591115, 751.21, 3533.4394022877746, 14.38924414518223, 230.68925136921567, 3533.4431522474183, \"['sapphire']\")\n",
      "(0.0, 18.005153870991126, 1055.9, 1438.2353910168954, 36.01030774198225, 37.52069967153426, 1438.2350582627107, \"['FormicAcid']\")\n",
      "(0.841160515588578, 29.158867423907406, 1217.6, 20781.336417309234, 58.31773484781481, 243.99745673788982, 20534.06786697443, \"['FormicAcid']\")\n",
      "(0.9999999958720391, 15.84659208464821, 1400.1, 15275.126734146714, 31.69318416929642, 306.830884253449, 15166.600231050901, \"['FormicAcid']\")\n",
      "(1.0517497753603777e-05, 47.91728201006456, 1649.3, 35138.18858345742, 95.83456402012912, 344.4478794033547, 35138.17791152145, \"['H2O']\")\n",
      "(0.17905767814449236, 26.21701236943559, 1714.9, 25828.109828259985, 52.43402473887118, 436.04210682097636, 25776.73308522814, \"['FormicAcid']\")\n",
      "(0.9999999835630924, 3.7717551873163035, 2137.9, 285.9437174244525, 7.543510374632607, 24.13165975513254, 285.5213391672587, \"['CarbonMonoxide']\")\n",
      "(0.4896056375696482, 3.6081206476853893, 2328.9, 255.9545663544563, 7.216241295370779, 28.062412623082338, 255.75275214220022, \"['Unassigned']\")\n",
      "(1.609823385706477e-15, 21.97860910902188, 2948.2, 3553.088421439357, 43.95721821804376, 75.93528100570454, 3553.088910320359, \"['FormicAcid']\")\n",
      "(0.5530107036199006, 79.80955569124941, 3185.1, 53203.06147074777, 159.61911138249883, 257.3093153411256, 41554.5553534982, \"['H2O']\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type \"<class 'numpy.ndarray'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7e87867aed64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhdf5_expfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataimport_ML_df-Copy2.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeak_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_expfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'300C/25s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_calfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-9303f862f81c>\u001b[0m in \u001b[0;36mpeak_assignment\u001b[0;34m(unknownhdf5_filename, key, knownhdf5_filename, precision, exportlabelinput, plot)\u001b[0m\n\u001b[1;32m    135\u001b[0m     df = pd.concat(frames,axis=1, join='outer', join_axes=None, ignore_index=False,\n\u001b[1;32m    136\u001b[0m               \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m               copy=True,sort=True)\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    284\u001b[0m                        \u001b[0;34m' only pd.Series, pd.DataFrame, and pd.Panel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                        ' (deprecated) objs are valid'.format(type(obj)))\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type \"<class 'numpy.ndarray'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid"
     ]
    }
   ],
   "source": [
    "hdf5_calfilename = 'calibration_data2.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy2.hdf5'\n",
    "\n",
    "df = peak_assignment(hdf5_expfilename, '300C/25s', hdf5_calfilename, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'calibration_data2.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy1.hdf5'\n",
    "\n",
    "frames = []\n",
    "for i, key in enumerate(key_list):\n",
    "    df = peakidentify.peak_assignment(hdf5_expfilename, '300C/25s', hdf5_calfilename, 10)\n",
    "    frames.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "         keys=None, levels=None, names=None, verify_integrity=False,\n",
    "         copy=True,sort=True)\n",
    "\n",
    "\n",
    "dat = pd.DataFrame()\n",
    "\n",
    "\n",
    "dat['fraction']= result[:][0]\n",
    "dat['sigma']= result[:][1]\n",
    "# dat['center']= result[:][2]\n",
    "dat['amplitude']= result[:][3]\n",
    "dat['fwhm']= result[:][4]\n",
    "dat['height']= result[:][5]\n",
    "dat['auc']= result[:][6]\n",
    "dat['labelencoded']=le.fit_transform(result[:][7])\n",
    "dat['labels']= result[:][7]\n",
    "dat['center']= result[:][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key_list in 'dataimport_ML_df-Copy1.hdf5':\n",
    "    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Step 5: Calculate Molar Decomposition\n",
    "\n",
    "To define the molar decomposition the area of the experimental data will be divided by the calibration data's area. This value will be the molar amount of the substance at the given experimental temperature and resonance time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Plot Molar Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Molar Decomposition with Reported Literature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
