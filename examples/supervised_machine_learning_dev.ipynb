{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import lineid_plot\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import datavis\n",
    "from ramandecompy import dataimport\n",
    "from ramandecompy import machine_learning\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ../ramandecompy/tests/test_files/water.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/Methane_Baseline_Calibration.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/CO2_100wt%.csv fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/sapphire.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/Propane_test.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/Ethane_test.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n",
      "Data from ../ramandecompy/tests/test_files/Acetaldehyde_test.xlsx fit with compound pseudo-Voigt model. Results saved to supervised_calibration_dataset.hdf5.\n"
     ]
    }
   ],
   "source": [
    "dataprep.new_hdf5('supervised_calibration_dataset')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/water.xlsx', 'water')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx', 'hydrogen')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Methane_Baseline_Calibration.xlsx', 'methane')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx', 'carbon monoxide')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/CO2_100wt%.csv', 'carbon dioxide')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/sapphire.xlsx', 'sapphire')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Propane_test.xlsx', 'propane')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Ethane_test.xlsx', 'ethane')\n",
    "dataprep.add_calibration('supervised_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Acetaldehyde_test.xlsx', 'acetaldehyde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** supervised_calibration_dataset.hdf5 ****\n",
      "\u001b[1macetaldehyde\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    Peak_05\n",
      "|    Peak_06\n",
      "|    Peak_07\n",
      "|    Peak_08\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mcarbon dioxide\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mcarbon monoxide\u001b[0m\n",
      "|    Peak_01\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1methane\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mhydrogen\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mmethane\u001b[0m\n",
      "|    Peak_01\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mpropane\u001b[0m\n",
      "|    Peak_01\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1msapphire\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n",
      "\u001b[1mwater\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    counts\n",
      "|    residuals\n",
      "|    wavenumber\n"
     ]
    }
   ],
   "source": [
    "dataprep.view_hdf5('supervised_calibration_dataset.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated calibration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('supervised_calibration_interp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** supervised_calibration_interp.hdf5 ****\n"
     ]
    }
   ],
   "source": [
    "dataprep.view_hdf5('supervised_calibration_interp.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_interpfilename = 'supervised_calibration_interp.hdf5'\n",
    "# first a function that will return a normalized interpolated spectra\n",
    "target_compound = 'water'\n",
    "# compound = 'water'\n",
    "hdf5_calfilename = 'supervised_calibration_dataset.hdf5'\n",
    "spectra_count = 5\n",
    "def interpolated_spectra(hdf5_interpfilename, hdf5_calfilename, spectra_count):\n",
    "    hdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "    # get list of compounds from hdf5 file\n",
    "    y_data_list = []\n",
    "    x_data_list = []\n",
    "    \n",
    "    compound_list = list(hdf5.keys())\n",
    "    print(compound_list)\n",
    "    for _, target_compound in enumerate(compound_list):\n",
    "        x_data, y_data, labels = machine_learning.generate_spectra_dataset(hdf5_calfilename, target_compound, spectra_count)\n",
    "        y_data_list.append(y_data)\n",
    "        x_data_list.append(x_data)\n",
    "        for i, label in enumerate(labels):\n",
    "            interpdf = machine_learning.combine_experiment(hdf5_interpfilename, 'interp_'+target_compound, x_data, y_data, label, i) \n",
    "            \n",
    "    return interpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acetaldehyde', 'carbon dioxide', 'carbon monoxide', 'ethane', 'hydrogen', 'methane', 'propane', 'sapphire', 'water']\n",
      "interp_acetaldehyde/0/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_acetaldehyde/1/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_acetaldehyde/2/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_acetaldehyde/3/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_acetaldehyde/4/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon dioxide/0/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon dioxide/1/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon dioxide/2/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon dioxide/3/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon dioxide/4/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon monoxide/0/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon monoxide/1/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon monoxide/2/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon monoxide/3/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_carbon monoxide/4/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_ethane/0/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_ethane/1/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_ethane/2/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_ethane/3/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_ethane/4/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_hydrogen/0/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_hydrogen/1/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_hydrogen/2/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n",
      "interp_hydrogen/3/residuals\n",
      "Data from fit with compound pseudo-Voigt model.\n",
      "          Results saved to supervised_calibration_interp.hdf5.\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "interpdf = interpolated_spectra(hdf5_interpfilename, hdf5_calfilename, spectra_count)\n",
    "frames.append(interpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = machine_learning.keyfinder(hdf5_interpfilename)\n",
    "print(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "      keys=None, levels=None, names=None, verify_integrity=False,\n",
    "      copy=True,sort=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE your hdf5 file before continuing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'supervised_calibration_dataset.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'supervised_calibration_interp.hdf5'\n",
    "frames = []\n",
    "for i,key in enumerate(key_list):\n",
    "    df =peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 10, plot =False)\n",
    "    frames.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn import preprocessing\n",
    "# create the Labelencoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "cal['fraction']= result[:][0]\n",
    "cal['sigma']= result[:][1]\n",
    "# dat['center']= result[:][2]\n",
    "cal['amplitude']= result[:][3]\n",
    "cal['fwhm']= result[:][4]\n",
    "cal['height']= result[:][5]\n",
    "cal['auc']= result[:][6]\n",
    "cal['labelencoded']=le.fit_transform(result[:][7])\n",
    "cal['labels']= result[:][7]\n",
    "cal['center']= result[:][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('dataimport_ML_df-Copy1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = machine_learning.keyfinder('dataimport_ML_df-Copy1.hdf5')\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'supervised_calibration_dataset.hdf5' \n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy1.hdf5'\n",
    "frames = []\n",
    "for _, key in enumerate(key_list):\n",
    "    df =peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 10, plot =False)\n",
    "    frames.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn import preprocessing\n",
    "# create the Labelencoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "dat['fraction']= result[:][0]\n",
    "dat['sigma']= result[:][1]\n",
    "# dat['center']= result[:][2]\n",
    "dat['amplitude']= result[:][3]\n",
    "dat['fwhm']= result[:][4]\n",
    "dat['height']= result[:][5]\n",
    "dat['auc']= result[:][6]\n",
    "dat['labelencoded']=le.fit_transform(result[:][7])\n",
    "dat['labels']= result[:][7]\n",
    "dat['center']= result[:][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=['fraction','sigma','amplitude','fwhm','height','auc','labelencoded']\n",
    "X_train=cal[inputs]\n",
    "y_train = cal['center']\n",
    "X_test=dat[inputs]\n",
    "y_test= dat['center']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide and extract test/train and validation data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(y_data_list[0], labels, test_size=0.2)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "Z = logreg.predict(X_test)\n",
    "\n",
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.scatter(x_data, y_data, label='data')\n",
    "# ax.scatter(X_train, y_train, label='train')\n",
    "# ax.scatter(X_test, y_test, label='teste')\n",
    "# ax.scatter(X_test, regr.predict(X_test), label='predicted')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 3))\n",
    "# y = dat['fraction'].values\n",
    "# x = dat['height'].values\n",
    "# # Create the linear regression model\n",
    "# LogRegr = linear_model.LogisticRegression() \n",
    "# # plotting the balance\n",
    "# ax.scatter(dat['height'], dat['fraction'], label = 'balance default',color='r')\n",
    "\n",
    "# # fit the linear model \n",
    "# LogRegr.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
    "# xres = np.array(x).reshape(-1,1)\n",
    "# plt.plot(x,LogRegr.predict_proba(xres)[:,1],'o',label = 'fit')\n",
    "# # Find the coefficients B0 and B1\n",
    "# print('B0, B1: ',LogRegr.intercept_, LogRegr.coef_[0])\n",
    "# ax.set_ylabel('Probability Default')\n",
    "# ax.set_xlabel('Balance')\n",
    "# ax.set_title('Probability Default vs. Balance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numdescriptors = 8\n",
    "# train linear model of descriptors\n",
    "yrange =  np.arange(1,numdescriptors,1)\n",
    "for Y in yrange:\n",
    "    MLR=linear_model.LinearRegression()\n",
    "    MLR.fit(train[train.columns.values[0:Y]],train[train.columns.values[Y]])\n",
    "    # WE are going to train using the first 8 values\n",
    "\n",
    "    # make predictions on test and train set \n",
    "    trainpred=MLR.predict(train[train.columns.values[0:Y]])\n",
    "    # predict the outputs using the training dataset\n",
    "    testpred=MLR.predict(test[train.columns.values[0:Y]])\n",
    "    # predict using test dataset\n",
    "    #make parity plot \n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([0,maxlimit]);\n",
    "    plt.ylim([0,maxlimit]);\n",
    "    print('This is for Y ='+str(Y))\n",
    "    plt.scatter(train[train.columns.values[Y]],trainpred, label='Training')\n",
    "    plt.scatter(test[train.columns.values[Y]],testpred,color='r', label='Test')\n",
    "    plt.plot([0,maxlimit],[0,maxlimit],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')\n",
    "\n",
    "    #calculate the test and train error\n",
    "    \n",
    "    print(\"Train error\",mean_squared_error(train[train.columns.values[Y]],trainpred)) # MSE of training dataset\n",
    "    print(\"Test error\",mean_squared_error(test[train.columns.values[Y]],testpred))\n",
    "    # usually the training error is less than the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
