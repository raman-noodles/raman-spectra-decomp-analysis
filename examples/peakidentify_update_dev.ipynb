{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeakIdentify - Example and Functionality\n",
    "\n",
    "***\n",
    "\n",
    "This notebook walks you through the usage and functionality of the updated peakidentify function. First, the functionality will be shown by just calling the wrapper function, but subsequent examples will explore the sub-functions and explain their usage. \n",
    "\n",
    "This function was created by the Raman Noodles Dev Team, and we hope you find it to your liking. Feel free to leave issues for suggested improvments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 THINGS TO CONSIDER\n",
    "#### NEED TO IMPROVE Visualization and Documentation\n",
    "#### NEED TO IMPROVE Score sort function and optimize using regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 0.A: Data Input\n",
    "\n",
    "First, we'll generate the dataset that we will use to explore this functionality. This data will be downloaded from _____\n",
    "\n",
    ". In order to generate an \"unknown spectrum\" that we will be attempting to fit, ____________, and feed that in as our unknown dataset. For further explanation of the dataprep or spectrafit packages, refer to the Jupyter notebooks which present examples of their usage, also found in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import lineid_plot\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import datavis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pair of known spectra from our calibration list and put it in hdf5 format with the dataprep module. See dataprep example for more details. The functions new_hdf5 and add_calibration should only be called once or else an File Already exists error will be raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you add a new hdf5 file you use this function below\n",
    "dataprep.new_hdf5('peakidentify_calibration_file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_calibration_file.hdf5') #already have this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_calibration('peakidentify_calibration_file.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                          label='Hydrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_calibration('peakidentify_calibration_file.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "                          label='Methane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_calibration('peakidentify_calibration_file.hdf5','CO2_100wt%.csv',label='CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the components in the file -- these are the pure component spectra only for calibration use\n",
    "test_example = h5py.File('peakidentify_calibration_file.hdf5', 'r+')\n",
    "list(test_example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the values of the files\n",
    "list(test_example.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_example) #checking the type of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotted known spectra with detected peaks\n",
    "datavis.plot_fit('peakidentify_calibration_file.hdf5', 'Hydrogen') #plotting teh hydrogen after going through with dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datavis.plot_fit('peakidentify_calibration_file.hdf5', 'Methane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datavis.plot_fit('peakidentify_calibration_file.hdf5', 'CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('peakidentify_experiment_file')\n",
    "dataprep.add_experiment('peakidentify_experiment_file.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_experiment_file.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datavis.plot_fit('peakidentify_experiment_file.hdf5', '300C/25s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('peakidentify_label_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_experiment('peakidentify_label_test.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_label_test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename = 'peakidentify_label_test.hdf5'\n",
    "temp = 300\n",
    "time = 25\n",
    "peak = 'Peak_01'\n",
    "label = '[Hydrogen]'\n",
    "\n",
    "peakidentify.add_label(hdf5_filename, temp, time, peak, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5 = h5py.File(hdf5_filename, 'r+') \n",
    "foo = hdf5['{}C/{}s/{}'.format(temp, time, peak)][0]\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [1,2,3,4,5,6,7,8]\n",
    "foo = [tuple(result[:7]),]\n",
    "foo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tuple(result[:7])\n",
    "data_array = np.array(data, dtype='<f8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datavis.plot_fit('peakidentify_experiment_file.hdf5', '300C/25s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 0.B: Test Driven Development - test functions (will be deleted for final version)\n",
    "06032019 tests have correct directories and nosetests passes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('peakidentify_experiment_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_experiment('peakidentify_experiment_test.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('peakidentify_calibration_test')\n",
    "dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                          label='Hydrogen')\n",
    "dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "                          label='Methane')\n",
    "dataprep.add_calibration('peakidentify_calibration_test.hdf5','../ramandecompy/tests/test_files/CO2_100wt%.csv',label='CO2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module used to unit test the functionality and outputs of the peakidentify.py module\n",
    "\"\"\"\n",
    "# IMPORTING MODULES\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "\n",
    "\n",
    "\n",
    "def test_peak_assignment():\n",
    "    \"\"\"This function tests the operation of the peak_assignment\n",
    "    function in peakidentify.py\"\"\"\n",
    "    #First, generate a testing dataset.\n",
    "#     dataprep.new_hdf5('peakidentify_calibration_test')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "#                              label='Hydrogen')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "#                              label='Methane')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/CO2_100wt%.csv',\n",
    "#                              label='CO2')\n",
    "\n",
    "#     dataprep.new_hdf5('peakidentify_experiment_test')\n",
    "#     dataprep.add_experiment('peakidentify_experiment_test.hdf5',\n",
    "#                             '../tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "    hdf5_calfilename = 'peakidentify_calibration_test.hdf5'\n",
    "    hdf5_expfilename = 'peakidentify_experiment_test.hdf5'\n",
    "    temp = 300\n",
    "    time = 25\n",
    "    calhdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "    exphdf5 = h5py.File(hdf5_expfilename, 'r+')\n",
    "    unknown_x = list(exphdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    unknown_y = list(exphdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    precision = 50\n",
    "    #Various try statements to make sure that bad inputs are handled correctly.\n",
    "\n",
    "    try:\n",
    "        peakidentify.peak_assignment(hdf5_expfilename,\n",
    "                                     temp, time, hdf5_calfilename,\n",
    "                                     precision, False, False)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid known_compound_list was passed to the function, \"\n",
    "              \"and it was handled well with a TypeError.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.peak_assignment(hdf5_expfilename,\n",
    "                                     temp, time, hdf5_calfilename,\n",
    "                                     'precision', False, False)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid precision value was passed to the function, and \"\n",
    "              \"it was handled well with a TypeError.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.peak_assignment(hdf5_expfilename,\n",
    "                                     temp, time, hdf5_calfilename,\n",
    "                                     precision, 'False', False)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid export label value was passed to the function, and it \"\n",
    "              \"was handled well with a TypeError.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.peak_assignment(hdf5_expfilename,\n",
    "                                     temp, time, hdf5_calfilename,\n",
    "                                     precision, False, 'False')\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid plot value was passed to the function, and it \"\n",
    "              \"was handled well with a TypeError.\")\n",
    "\n",
    "    exphdf5.close()\n",
    "    calhdf5.close()\n",
    "#     os.remove('peakidentify_calibration_test.hdf5')\n",
    "#     os.remove('peakidentify_experiment_test.hdf5')\n",
    "\n",
    "def test_compare_unknown_to_known():\n",
    "    \"\"\"This function tests the operation of the compare_unknown_to_known\n",
    "    function in peakidentify.py\"\"\"\n",
    "    #First, generate a testing dataset.\n",
    "#     dataprep.new_hdf5('peakidentify_calibration_test')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "#                              label='Hydrogen')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "#                              label='Methane')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/CO2_100wt%.csv',\n",
    "#                              label='CO2')\n",
    "\n",
    "#     dataprep.new_hdf5('peakidentify_experiment_test')\n",
    "#     dataprep.add_experiment('peakidentify_experiment_test.hdf5',\n",
    "#                             '../tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "    hdf5_calfilename = 'peakidentify_calibration_test.hdf5'\n",
    "    hdf5_expfilename = 'peakidentify_experiment_test.hdf5'\n",
    "    temp = 300\n",
    "    time = 25\n",
    "    calhdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "    exphdf5 = h5py.File(hdf5_expfilename, 'r+')\n",
    "    unknown_x = list(exphdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    unknown_y = list(exphdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    known_compound_list = list(calhdf5.keys())\n",
    "    precision = 50\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    unknown_peaks = []\n",
    "    for i, _ in enumerate(list(exphdf5['{}C/{}s'.format(temp, time)])[:-3]):\n",
    "        if i < 9:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_0{}'.format(temp,\n",
    "                                                                        time,\n",
    "                                                                        i+1)])[0][2])\n",
    "        else:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_{}'.format(temp,\n",
    "                                                                       time,\n",
    "                                                                       i+1)])[0][2])\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        for _, peak in enumerate(list(calhdf5[known_compound_list[i]])[:-3]):\n",
    "            known_peaks_list.append(list(calhdf5['{}/{}'.format(known_compound_list[i],\n",
    "                                                                peak)])[0][2])\n",
    "        known_peaks.append(known_peaks_list[i])\n",
    "\n",
    "    try:\n",
    "        peakidentify.compare_unknown_to_known(1, known_peaks, precision)\n",
    "    except TypeError:\n",
    "        print(\"An invalid unknown_peaks value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.compare_unknown_to_known(unknown_peaks, 'known_peaks', precision)\n",
    "    except TypeError:\n",
    "        print(\"An invalid known_peaks value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.compare_unknown_to_known(unknown_peaks, known_peaks, 'precision')\n",
    "    except TypeError:\n",
    "        print(\"An invalid precision value was passed to the function, and \"\n",
    "              \"was handled correctly.\")\n",
    "\n",
    "    #After testing for resilience to unexpected inputs, now ensure\n",
    "    #outputs are performing correctly\n",
    "\n",
    "    #First, make sure function is returning the list.\n",
    "    assert isinstance(peakidentify.compare_unknown_to_known(\n",
    "        unknown_peaks, known_peaks, precision), np.ndarray), (\"\"\"Function\n",
    "        is not returning list\"\"\")\n",
    "\n",
    "    #Compare one set of peaks to itself. The full association matrix\n",
    "    #should have all values = 1.\n",
    "    self_comp = np.mean(peakidentify.compare_unknown_to_known(known_peaks,\n",
    "                                                              known_peaks,\n",
    "                                                              precision))\n",
    "    assert self_comp == 1, (\"Peak Assignment Error. Comparison of compound \"\n",
    "                            \"against itself should find all peaks.\")\n",
    "\n",
    "    dif_comp = np.mean(peakidentify.compare_unknown_to_known([1, 3, 6],\n",
    "                                                             [1000, 2000, 5000],\n",
    "                                                             precision))\n",
    "    assert dif_comp == 0, (\"Peak Assignment Error. Passed values should \"\n",
    "                           \"have no matching assignments.\")\n",
    "\n",
    "    exphdf5.close()\n",
    "    calhdf5.close()\n",
    "#     os.remove('peakidentify_calibration_test.hdf5')\n",
    "#     os.remove('peakidentify_experiment_test.hdf5')\n",
    "\n",
    "def test_peak_position_comparisons():\n",
    "    \"\"\"This function tests the operation of the peak_position_comparisons\n",
    "    function in peakidentify. Said function returns a list of strings that\n",
    "    contain text assignments of each peak in the unknown spectrum.\"\"\"\n",
    "\n",
    "    #First, generate a testing dataset.\n",
    "#     dataprep.new_hdf5('peakidentify_calibration_test')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "#                              label='Hydrogen')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "#                              label='Methane')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/CO2_100wt%.csv',\n",
    "#                              label='CO2')\n",
    "#     dataprep.new_hdf5('peakidentify_experiment_test')\n",
    "#     dataprep.add_experiment('peakidentify_experiment_test.hdf5',\n",
    "#                             '../tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "    hdf5_calfilename = 'peakidentify_calibration_test.hdf5'\n",
    "    hdf5_expfilename = 'peakidentify_experiment_test.hdf5'\n",
    "    temp = 300\n",
    "    time = 25\n",
    "    calhdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "    exphdf5 = h5py.File(hdf5_expfilename, 'r+')\n",
    "    unknown_x = list(exphdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    unknown_y = list(exphdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    known_compound_list = list(calhdf5.keys())\n",
    "    precision = 50\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    association_matrix = []\n",
    "    unknown_peaks = []\n",
    "    for i, _ in enumerate(list(exphdf5['{}C/{}s'.format(temp, time)])[:-3]):\n",
    "        if i < 9:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_0{}'.format(temp,\n",
    "                                                                        time,\n",
    "                                                                        i+1)])[0][2])\n",
    "        else:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_{}'.format(temp,\n",
    "                                                                       time,\n",
    "                                                                       i+1)])[0][2])\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    num_peaks_list = []\n",
    "    association_matrix = []\n",
    "    split__index_list = []\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        num_peaks_list.append(len(list(calhdf5[known_compound_list[i]])[:-3]))\n",
    "        split__index_list.append(sum(num_peaks_list))\n",
    "        for j, peak in enumerate(list(calhdf5[known_compound_list[i]])[:-3]):\n",
    "            # Need to separate known peaks to make a list of two separate lists\n",
    "            # to perform custom list split using list comprehension + zip()\n",
    "            # and split_index_list\n",
    "            known_peaks_list.append(list(calhdf5['{}/{}'.format(known_compound_list[i],\n",
    "                                                                peak)])[0][2])\n",
    "            result = [known_peaks_list[i : j] for i, j in zip([0] + split__index_list,\n",
    "                                                              split__index_list +\n",
    "                                                              [None])]\n",
    "        known_peaks.append(result)\n",
    "        association_matrix.append(peakidentify.compare_unknown_to_known(\n",
    "            unknown_peaks, known_peaks[i][i], precision))\n",
    "\n",
    "    #Then, test error handling of bad inputs for the function.\n",
    "    try:\n",
    "        peakidentify.peak_position_comparisons(1, known_peaks,\n",
    "                                               association_matrix,\n",
    "                                               hdf5_calfilename)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid unknown_peaks value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.peak_position_comparisons(unknown_peaks,\n",
    "                                               'known_peaks',\n",
    "                                               association_matrix,\n",
    "                                               hdf5_calfilename)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid known_peaks value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.peak_position_comparisons(unknown_peaks,\n",
    "                                               known_peaks,\n",
    "                                               'association_matrix',\n",
    "                                               hdf5_calfilename)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"An invalid association_matrix value was passed to the function,\"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    #Check to make sure the function is returning a list.\n",
    "    assert isinstance(peakidentify.peak_position_comparisons(\n",
    "        unknown_peaks, known_peaks,\n",
    "        association_matrix, hdf5_calfilename), list), \"\"\"The function is\n",
    "        not returning a list.\"\"\"\n",
    "\n",
    "    #Test a call that says that no peaks have associations\n",
    "    association_matrix_0 = []\n",
    "\n",
    "    association_matrix_0.append(peakidentify.compare_unknown_to_known(\n",
    "        known_peaks[0][0],\n",
    "        known_peaks[1][0],\n",
    "        precision))\n",
    "\n",
    "    zero_output = peakidentify.peak_position_comparisons(known_peaks[0][0],\n",
    "                                                         [known_peaks[1][0]],\n",
    "                                                         association_matrix_0,\n",
    "                                                         hdf5_calfilename)[0]\n",
    "\n",
    "    assert zero_output[0] == 'CO2', \"\"\"The function is not properly\n",
    "    handling unassigned peaks.\"\"\"\n",
    "\n",
    "    #Test the function to make sure that it has the right functionality\n",
    "    association_matrix = []\n",
    "    #Generate a matrix with all associations equal to 1\n",
    "    association_matrix.append(peakidentify.compare_unknown_to_known(\n",
    "        known_peaks[0][0],\n",
    "        known_peaks[0][0],\n",
    "        precision))\n",
    "\n",
    "    #change the middle index to 0\n",
    "    association_matrix[0][1] = 0\n",
    "    test_peak_labels = peakidentify.peak_position_comparisons(known_peaks[0][0],\n",
    "                                                              [known_peaks[0][0]],\n",
    "                                                              association_matrix,\n",
    "                                                              hdf5_calfilename)\n",
    "    print(test_peak_labels[0][0])\n",
    "    print(test_peak_labels[1][0])\n",
    "\n",
    "    assert test_peak_labels[0][0] == 'CO2', \"\"\"The funciton is\n",
    "    not correctly assigning peaks when association matrix = 1\"\"\"\n",
    "    assert test_peak_labels[1][0] == 'Unassigned', \"\"\"The function is\n",
    "    not correctly handling a lack of peak assignments\"\"\"\n",
    "\n",
    "    exphdf5.close()\n",
    "    calhdf5.close()\n",
    "#     os.remove('peakidentify_calibration_test.hdf5')\n",
    "#     os.remove('peakidentify_experiment_test.hdf5')\n",
    "\n",
    "def test_percentage_of_peaks_found():\n",
    "    \"\"\"This function tests the operation of the\n",
    "    percentage_of_peaks_found function in peakidentify.py\"\"\"\n",
    "\n",
    "    #First, generate a testing dataset.\n",
    "#     dataprep.new_hdf5('peakidentify_calibration_test')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "#                              label='Hydrogen')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "#                              label='Methane')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/CO2_100wt%.csv',\n",
    "#                              label='CO2')\n",
    "\n",
    "#     dataprep.new_hdf5('peakidentify_experiment_test')\n",
    "#     dataprep.add_experiment('peakidentify_experiment_test.hdf5',\n",
    "#                             '../tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "    hdf5_calfilename = 'peakidentify_calibration_test.hdf5'\n",
    "    hdf5_expfilename = 'peakidentify_experiment_test.hdf5'\n",
    "    temp = 300\n",
    "    time = 25\n",
    "    calhdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "    exphdf5 = h5py.File(hdf5_expfilename, 'r+')\n",
    "    unknown_x = list(exphdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    unknown_y = list(exphdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    known_compound_list = list(calhdf5.keys())\n",
    "    precision = 50\n",
    "\n",
    "    unknown_peaks = []\n",
    "    for i, _ in enumerate(list(exphdf5['{}C/{}s'.format(temp, time)])[:-3]):\n",
    "        if i < 9:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_0{}'.format(temp,\n",
    "                                                                        time,\n",
    "                                                                        i+1)])[0][2])\n",
    "        else:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_{}'.format(temp,\n",
    "                                                                       time,\n",
    "                                                                       i+1)])[0][2])\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    num_peaks_list = []\n",
    "    association_matrix = []\n",
    "    split__index_list = []\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        num_peaks_list.append(len(list(calhdf5[known_compound_list[i]])[:-3]))\n",
    "        split__index_list.append(sum(num_peaks_list))\n",
    "        for j, peak in enumerate(list(calhdf5[known_compound_list[i]])[:-3]):\n",
    "            # Need to separate known peaks to make a list of two separate lists\n",
    "            # to perform custom list split using list comprehension + zip()\n",
    "            # and split_index_list\n",
    "            known_peaks_list.append(list(calhdf5['{}/{}'.format(known_compound_list[i],\n",
    "                                                                peak)])[0][2])\n",
    "            result = [known_peaks_list[i : j] for i, j in zip([0] + split__index_list,\n",
    "                                                              split__index_list +\n",
    "                                                              [None])]\n",
    "        known_peaks.append(result)\n",
    "        association_matrix.append(peakidentify.compare_unknown_to_known(\n",
    "            unknown_peaks, known_peaks[i][i], precision))\n",
    "\n",
    "    #Test for input error handling.\n",
    "\n",
    "    try:\n",
    "        peakidentify.percentage_of_peaks_found([[0], [1], [2], [3]],\n",
    "                                               association_matrix,\n",
    "                                               hdf5_calfilename)\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error when a list of ints\n",
    "        was input instead of the known_peaks list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.percentage_of_peaks_found(1, association_matrix,\n",
    "                                               hdf5_calfilename)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error when an int\n",
    "        was input instead of the known_peaks list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.percentage_of_peaks_found(known_peaks, 1,\n",
    "                                               known_compound_list,\n",
    "                                               hdf5_calfilename)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error when an int\n",
    "        was input instead of the association matrix\"\"\")\n",
    "\n",
    "\n",
    "    #Test to make sure function returns a dictionary.\n",
    "    assert isinstance(peakidentify.percentage_of_peaks_found(\n",
    "        known_peaks,\n",
    "        association_matrix,\n",
    "        hdf5_calfilename), dict), \"\"\"The function is not\n",
    "        returning a dictionary.\"\"\"\n",
    "\n",
    "#     #Test for function output.\n",
    "    co2_peaks = []\n",
    "    key = 'CO2'\n",
    "    for _, peak in enumerate(list(calhdf5[key])[:-3]):\n",
    "        co2_peaks.append(list(calhdf5['{}/{}'.format(key, peak)])[0][2])\n",
    "    print(co2_peaks)\n",
    "    co2_dict_0 = peakidentify.percentage_of_peaks_found([co2_peaks, [0], [0]],\n",
    "                                                        [[0, 0], [0], [0]],\n",
    "                                                        hdf5_calfilename)\n",
    "    assert co2_dict_0[key] == 0, \"\"\"The function is not correctly\n",
    "    calculating percentages when no peaks are found\"\"\"\n",
    "\n",
    "    co2_dict_1 = peakidentify.percentage_of_peaks_found([co2_peaks, [1], [1]],\n",
    "                                                        [[1, 1], [1], [1]],\n",
    "                                                        hdf5_calfilename)\n",
    "    assert co2_dict_1[key] == 100, \"\"\"The function is not correctly\n",
    "    calculating percentages when all peaks are found\"\"\"\n",
    "\n",
    "    exphdf5.close()\n",
    "    calhdf5.close()\n",
    "#     os.remove('peakidentify_calibration_test.hdf5')\n",
    "#     os.remove('peakidentify_experiment_test.hdf5')\n",
    "\n",
    "def test_plotting_peak_assignments():\n",
    "    \"\"\"This function tests the operation of the peak_assignment\n",
    "    function in peakidentify.py\"\"\"\n",
    "    #First, generate a testing dataset.\n",
    "#     dataprep.new_hdf5('peakidentify_calibration_test')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "#                              label='Hydrogen')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/Methane_Baseline_Calibration.xlsx',\n",
    "#                              label='Methane')\n",
    "#     dataprep.add_calibration('peakidentify_calibration_test.hdf5',\n",
    "#                              '../tests/test_files/CO2_100wt%.csv',\n",
    "#                              label='CO2')\n",
    "\n",
    "#     dataprep.new_hdf5('peakidentify_experiment_test')\n",
    "#     dataprep.add_experiment('peakidentify_experiment_test.hdf5',\n",
    "#                             '../tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "    hdf5_calfilename = 'peakidentify_calibration_test.hdf5'\n",
    "    hdf5_expfilename = 'peakidentify_experiment_test.hdf5'\n",
    "    temp = 300\n",
    "    time = 25\n",
    "    calhdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "    exphdf5 = h5py.File(hdf5_expfilename, 'r+')\n",
    "    unknown_x = list(exphdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    unknown_y = list(exphdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    known_compound_list = list(calhdf5.keys())\n",
    "    precision = 50\n",
    "\n",
    "    unknown_peaks = []\n",
    "    for i, _ in enumerate(list(exphdf5['{}C/{}s'.format(temp, time)])[:-3]):\n",
    "        if i < 9:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_0{}'.format(temp,\n",
    "                                                                        time,\n",
    "                                                                        i+1)])[0][2])\n",
    "        else:\n",
    "            unknown_peaks.append(list(exphdf5['{}C/{}s/Peak_{}'.format(temp,\n",
    "                                                                       time,\n",
    "                                                                       i+1)])[0][2])\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    num_peaks_list = []\n",
    "    association_matrix = []\n",
    "    split__index_list = []\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        num_peaks_list.append(len(list(calhdf5[known_compound_list[i]])[:-3]))\n",
    "        split__index_list.append(sum(num_peaks_list))\n",
    "        for j, peak in enumerate(list(calhdf5[known_compound_list[i]])[:-3]):\n",
    "            # Need to separate known peaks to make a list of two separate lists\n",
    "            # to perform custom list split using list comprehension + zip()\n",
    "            # and split_index_list\n",
    "            known_peaks_list.append(list(calhdf5['{}/{}'.format(known_compound_list[i],\n",
    "                                                                peak)])[0][2])\n",
    "            result = [known_peaks_list[i : j] for i, j in zip([0] + split__index_list,\n",
    "                                                              split__index_list +\n",
    "                                                              [None])]\n",
    "        known_peaks.append(result)\n",
    "        association_matrix.append(peakidentify.compare_unknown_to_known(\n",
    "            unknown_peaks, known_peaks[i][i], precision))\n",
    "    #Ok, so that generates a full association matrix that contains everything\n",
    "    #we need to assign peaks.\n",
    "    #Now, let's go through and actually assign text to peaks.\n",
    "    unknown_peak_assignments = peakidentify.peak_position_comparisons(\n",
    "        unknown_peaks, known_peaks, association_matrix, hdf5_calfilename)\n",
    "    peak_labels = []\n",
    "    for i, _ in enumerate(unknown_peak_assignments):\n",
    "        peak_labels.append(str(unknown_peak_assignments[i]))\n",
    "    #Test for input error handling.\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(1, unknown_y, unknown_peaks,\n",
    "                                               unknown_peak_assignments,\n",
    "                                               hdf5_expfilename,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error\n",
    "        when an int was input instead of the unknown_x list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x, 3, unknown_peaks,\n",
    "                                               unknown_peak_assignments,\n",
    "                                               hdf5_expfilename,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error when an int\n",
    "        was input instead of the unknown_y list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                               unknown_y,\n",
    "                                               'unknown_peaks',\n",
    "                                               unknown_peak_assignments,\n",
    "                                               hdf5_expfilename,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error when a string\n",
    "        was input instead of the unknown_peaks list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                               unknown_y,\n",
    "                                               unknown_peaks,\n",
    "                                               3,\n",
    "                                               hdf5_expfilename,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the error when an int\n",
    "        was input instead of the unknown_peak_assignments\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                               unknown_y,\n",
    "                                               unknown_peaks,\n",
    "                                               ['WATER', 23, 'CO'],\n",
    "                                               hdf5_expfilename,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the case when an int\n",
    "        was passed in the unknown_peak_assignment list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                               unknown_y,\n",
    "                                               unknown_peaks,\n",
    "                                               ['H', 23, 'CO2'],\n",
    "                                               hdf5_expfilename,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the case when an int\n",
    "        was passed in the unknown_peak_assignment list\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                               unknown_y,\n",
    "                                               unknown_peaks,\n",
    "                                               unknown_peak_assignments,\n",
    "                                               3,\n",
    "                                               hdf5_calfilename,\n",
    "                                               temp, time, peak_labels)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the case when an int\n",
    "        was passed in the hdf5_filename\"\"\")\n",
    "\n",
    "    try:\n",
    "        peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                               unknown_y,\n",
    "                                               unknown_peaks,\n",
    "                                               unknown_peak_assignments,\n",
    "                                               hdf5_expfilename,\n",
    "                                               3,\n",
    "                                               temp, time, peak_labels)\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"\"\"The function correctly handled the case when an int\n",
    "        was passed in the hdf5_calfilename\"\"\")\n",
    "\n",
    "    exphdf5.close()\n",
    "    calhdf5.close()\n",
    "#     os.remove('peakidentify_calibration_test.hdf5')\n",
    "#     os.remove('peakidentify_experiment_test.hdf5')\n",
    "\n",
    "def test_add_label():\n",
    "    \"\"\"\n",
    "    Function that adds a label to a peak dataset in the hdf5 file\n",
    "    \"\"\"\n",
    "    dataprep.new_hdf5('peakidentify_add_label_test')\n",
    "    dataprep.add_experiment('peakidentify_add_label_test.hdf5',\n",
    "                            '../tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "    hdf5_filename = 'peakidentify_add_label_test.hdf5'\n",
    "    temp = 300\n",
    "    time = 25\n",
    "    peak = 'Peak_01'\n",
    "    label = '[Hydrogen]'\n",
    "    # open hdf5 file as read/write\n",
    "    hdf5 = h5py.File(hdf5_filename, 'r+')\n",
    "    try:\n",
    "        peakidentify.add_label('hdf5_filename', temp, time, peak, label)\n",
    "    except TypeError:\n",
    "        print(\"An invalid hdf5_filename was passed to the function, \"\n",
    "              \"and it was handled well with a TypeError.\")\n",
    "    try:\n",
    "        peakidentify.add_label(hdf5_filename, 'temp', time, peak, label)\n",
    "    except TypeError:\n",
    "        print(\"An invalid temp was passed to the function, \"\n",
    "              \"and it was handled well with a TypeError.\")\n",
    "    try:\n",
    "        peakidentify.add_label(hdf5_filename, temp, 'time', peak, label)\n",
    "    except TypeError:\n",
    "        print(\"An invalid time was passed to the function, \"\n",
    "              \"and it was handled well with a TypeError.\")\n",
    "    hdf5.close()\n",
    "    del hdf5\n",
    "#     os.remove('peakidentify_add_label_test.hdf5')\n",
    "    return\n",
    "\n",
    "def test_peak_1d_score():\n",
    "    \"\"\"Evaluates the functionality of the peak_1D_score function\"\"\"\n",
    "    # Initialize the test arguments\n",
    "    row_i = [0, 1]\n",
    "    row_j = [2, 1]\n",
    "    rowcat = row_i + row_j\n",
    "    arraya = np.array([[0, 1], [2, 1], [0, 3]])\n",
    "    arraycat = np.concatenate((arraya[0], arraya[2]))\n",
    "    precision = 50\n",
    "\n",
    "    # Run Bad Function for lists\n",
    "    try:\n",
    "        testscore = peakidentify.peak_1d_score(row_i, row_j, -1, precision)\n",
    "    except ValueError:\n",
    "        print(\"An invalid scoremax value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    # Run Bad Function for arrays\n",
    "    try:\n",
    "        arrayscore = peakidentify.peak_1d_score(arraya[0], arraya[2], -1, precision)\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"An invalid scoremax value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    # Running a good example\n",
    "    testscore = peakidentify.peak_1d_score(row_i, row_j, 1., precision)\n",
    "    arrayscore = peakidentify.peak_1d_score(arraya[0], arraya[2], 1, precision)\n",
    "\n",
    "    # make assertions\n",
    "    assert len(row_i) == len(row_j), 'Input lengths do not match'\n",
    "    assert len(arrayscore[0][:]) == len(arraycat), \"\"\"Output list length\n",
    "    different than concatenated lists length\"\"\"\n",
    "    for i in range(len(rowcat)):\n",
    "        assert 0 <= testscore[0][i] <= 1, 'Output value outside acceptable range'\n",
    "        assert 0 <= arrayscore[0][i] <= 1, 'Output value outside acceptable range'\n",
    "\n",
    "\n",
    "def test_score_max():\n",
    "    \"\"\"Evaluates the functionality of the score_max function\"\"\"\n",
    "    # Initialize the test arguments\n",
    "    k = 2\n",
    "    row_i = [0, 3]\n",
    "    row_j = [2, 1]\n",
    "    rowcat = row_i + row_j\n",
    "    arraya = np.array([[0, 1], [2, 1], [0, 3]])\n",
    "    precision = 50\n",
    "\n",
    "    arraycat = np.concatenate((arraya[0], arraya[1]))\n",
    "\n",
    "    # Run Function for lists\n",
    "    try:\n",
    "\n",
    "        maxscores = peakidentify.score_max(row_i, row_j, -1, precision)\n",
    "\n",
    "    except ValueError:\n",
    "\n",
    "        print(\"An invalid k value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "     # Run Function for arrays\n",
    "    try:\n",
    "\n",
    "        arrmaxscores = peakidentify.score_max(arraya[0], arraya[1], -1, precision)\n",
    "\n",
    "    except ValueError:\n",
    "\n",
    "        print(\"An invalid k value was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    # Run good examples\n",
    "    maxscores = peakidentify.score_max(row_i, row_j, k, precision)\n",
    "    arrmaxscores = peakidentify.score_max(arraya[0], arraya[1], k, precision)\n",
    "\n",
    "    # make assertions\n",
    "    assert len(arrmaxscores[0]) == len(arraycat), \"\"\"Output list length different\n",
    "    than concatenated lists length\"\"\"\n",
    "    for i, _ in enumerate(rowcat):\n",
    "        assert 0 <= arrmaxscores[0][i] <= 2, 'Output value outside acceptable range'\n",
    "        assert 0 <= maxscores[0][i] <= 2, 'Output value outside acceptable range'\n",
    "    for i, _ in enumerate(maxscores, 1):\n",
    "        assert maxscores[0][i-1] >= maxscores[0][-1], \"\"\"Output values are\n",
    "        less than the max value\"\"\"\n",
    "\n",
    "\n",
    "def test_score_sort():\n",
    "    \"\"\"Evaluates the functionality of the score_sort function\"\"\"\n",
    "    # Initialize the test arguments\n",
    "    row_i = [0, 1]\n",
    "    row_j = [2, 1]\n",
    "    rowcat = row_i + row_j\n",
    "    arraya = np.array([[0, 1], [2, 1], [0, 3]])\n",
    "    k = 2\n",
    "    precision = 50\n",
    "    arraycat = np.concatenate((arraya[0], arraya[1]))\n",
    "    # Run Previous Function to get max score normalization\n",
    "    maxscores = peakidentify.score_max(row_i, row_j, k, precision)\n",
    "\n",
    "    # Run Function for lists\n",
    "\n",
    "    try:\n",
    "        sortedscores = peakidentify.score_sort(row_i, row_j, max(maxscores[0]), precision)\n",
    "\n",
    "    except TypeError:\n",
    "\n",
    "        print(\"An invalid maxscores from score_max was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    # Run Function for arrays\n",
    "\n",
    "    try:\n",
    "\n",
    "        arrsortedscores = peakidentify.score_sort(arraya[0], arraya[1],\n",
    "                                                  max(maxscores[0]),\n",
    "                                                  precision)\n",
    "\n",
    "    except TypeError:\n",
    "\n",
    "        print(\"An invalid maxscores from score_max was passed to the function, \"\n",
    "              \"and was handled correctly.\")\n",
    "\n",
    "    # Run good examples\n",
    "    sortedscores = peakidentify.score_sort(row_i, row_j,\n",
    "                                           int(max(maxscores[0])),\n",
    "                                           precision)\n",
    "    arrsortedscores = peakidentify.score_sort(arraya[0], arraya[1],\n",
    "                                              int(max(maxscores[0])),\n",
    "                                              precision)\n",
    "    # make assertions\n",
    "    assert len(arraycat) == len(arrsortedscores[0][0]), \"\"\"Output list length\n",
    "    different than concatenated lists length\"\"\"\n",
    "    assert len(rowcat) == len(sortedscores[0][0]), \"\"\"Output list length\n",
    "    different than concatenated lists length\"\"\"\n",
    "    for i, _ in enumerate(sortedscores):\n",
    "        assert sortedscores[0][0][i] <= sortedscores[0][0][i+1], \"\"\"Output values\n",
    "        is sorted from smallest to largest\"\"\"\n",
    "        assert arrsortedscores[0][0][i] <= arrsortedscores[0][0][i+1], \"\"\"Output\n",
    "        values is sorted from smallest to largest\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peakidentify.py Function (will be deleted for final version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes in compounds from a dictionary from shoyu, and, using spectrafit,\n",
    "identifies peaks found in both the fed-in known spectra, as well as the unknown spectra\n",
    "to be analyzed. From that identification, it then classifies the peaks in the unknown\n",
    "spectra based on the fed-in known spectra.\n",
    " \"\"\"\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lineid_plot\n",
    "\n",
    "\n",
    "# Will probably need to create an additional function\n",
    "\n",
    "def peak_assignment(unknownhdf5_filename, temp, time, knownhdf5_filename,\n",
    "                    precision=50, exportlabelinput=True, plot=True):\n",
    "    \"\"\"This function is a wrapper function from which all classification\n",
    "    of peaks occurs.\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(unknownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `unknownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(unknownhdf5_filename)))\n",
    "    if not unknownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`unknownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + unknownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(temp, int):\n",
    "        raise TypeError(\"\"\"Passed value of `temp` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(temp)))\n",
    "    if not isinstance(time, int):\n",
    "        raise TypeError(\"\"\"Passed value of `time` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(time)))\n",
    "\n",
    "    if not isinstance(precision, (float, int)):\n",
    "        raise TypeError(\"\"\"Passed value of `precision` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(precision)))\n",
    "    if not isinstance(exportlabelinput, bool):\n",
    "        raise TypeError(\"\"\"Passed value of `exportlabelinput` is not a Boolean!\n",
    "        Instead, it is: \"\"\" + str(type(exportlabelinput)))\n",
    "    if not isinstance(plot, bool):\n",
    "        raise TypeError(\"\"\"Passed value of `plot` is not a Boolean!\n",
    "        Instead, it is: \"\"\" + str(type(plot)))\n",
    "    # open .hdf5\n",
    "    unhdf5 = h5py.File(unknownhdf5_filename, 'r+')\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r+')\n",
    "\n",
    "    #Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_compound_list)))\n",
    "    #Now we need to check the elements within the known_compound_list\n",
    "    #to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list` is not\n",
    "            a string! Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "\n",
    "    # extract spectra data\n",
    "    unknown_x = list(unhdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    unknown_y = list(unhdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "    unknown_x = np.asarray(unknown_x)\n",
    "    unknown_y = np.asarray(unknown_y)\n",
    "    #Lets identify the peaks in the unknown spectrum.\n",
    "    unknown_peaks = []\n",
    "    for i, peak in enumerate(list(unhdf5['{}C/{}s'.format(temp, time)])[:-3]):\n",
    "        try:\n",
    "            if i < 9:\n",
    "                unknown_peaks.append(list(unhdf5['{}C/{}s/Peak_0{}*'.format(temp,\n",
    "                                                                            time,\n",
    "                                                                            i+1)])[0][2])\n",
    "            else:\n",
    "                unknown_peaks.append(list(unhdf5['{}C/{}s/Peak_{}*'.format(temp,\n",
    "                                                                           time,\n",
    "                                                                           i+1)])[0][2])\n",
    "        except Exception as e:\n",
    "            #Normal peakassignment\n",
    "            print(\"\"\"Function did not receive adjusted peak.\n",
    "            The function continued to look for an normal peak.\"\"\")\n",
    "            if i < 9:\n",
    "                print(peak)\n",
    "                unknown_peaks.append(list(unhdf5['{}C/{}s/Peak_0{}'.format(temp,\n",
    "                                                                           time,\n",
    "                                                                           i+1)])[0][2])\n",
    "            else:\n",
    "                unknown_peaks.append(list(unhdf5['{}C/{}s/Peak_{}'.format(temp,\n",
    "                                                                          time,\n",
    "                                                                          i+1)])[0][2])\n",
    "            print('Peak_{}*'.format(i+1))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    #OK, next identify all of the peaks present in the known compound set.\n",
    "    #For efficiency, we'll also compare them against the unknown in the same for loop.\n",
    "    known_peaks = []\n",
    "    known_peaks_list = []\n",
    "    num_peaks_list = []\n",
    "    assignment_matrix = []\n",
    "    split__index_list = []\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        print(\"The peaks that we found for \"\n",
    "              + str(known_compound_list[i]) + \" are: \")\n",
    "        num_peaks_list.append(len(list(knhdf5[known_compound_list[i]])[:-3]))\n",
    "        split__index_list.append(sum(num_peaks_list))\n",
    "        for j, peak in enumerate(list(knhdf5[known_compound_list[i]])[:-3]):\n",
    "            print(list(knhdf5['{}/{}'.format(known_compound_list[i], peak)])[0][2])\n",
    "            # Need to separate known peaks to make a list of two separate lists\n",
    "            # to perform custom list split using list comprehension + zip() and split_index_list\n",
    "            known_peaks_list.append(list(knhdf5['{}/{}'.format(known_compound_list[i],\n",
    "                                                               peak)])[0][2])\n",
    "            result = [known_peaks_list[i : j] for i, j in zip([0] + split__index_list,\n",
    "                                                              split__index_list +\n",
    "                                                              [None])]\n",
    "        known_peaks.append(result)\n",
    "        assignment_matrix.append(compare_unknown_to_known(\n",
    "            unknown_peaks, known_peaks[i][i], precision))\n",
    "    #Ok, so that generates a full association matrix that contains everything\n",
    "    #we need to assign peaks.\n",
    "    #Now, let's go through and actually assign text to peaks.\n",
    "    unknown_peak_assignments = peak_position_comparisons(unknown_peaks,\n",
    "                                                         known_peaks,\n",
    "                                                         assignment_matrix,\n",
    "                                                         knownhdf5_filename)\n",
    "    print(unknown_peak_assignments)\n",
    "    peak_labels = []\n",
    "    for i, _ in enumerate(unknown_peak_assignments):\n",
    "        peak_labels.append(str(unknown_peak_assignments[i]))\n",
    "    for j, peak in enumerate(list(unhdf5['{}C/{}s'.format(temp, time)])[:-3]):\n",
    "        add_label(unknownhdf5_filename, temp, time, peak, peak_labels[j])\n",
    "    if plot:\n",
    "        plotting_peak_assignments(unknown_x,\n",
    "                                  unknown_y,\n",
    "                                  unknown_peaks,\n",
    "                                  unknown_peak_assignments,\n",
    "                                  unknownhdf5_filename,\n",
    "                                  knownhdf5_filename,\n",
    "                                  temp,\n",
    "                                  time,\n",
    "                                  peak_labels,\n",
    "                                  exportlabelinput)\n",
    "\n",
    "    percentages = percentage_of_peaks_found(known_peaks[len(known_compound_list)-1],\n",
    "                                            assignment_matrix,\n",
    "                                            knownhdf5_filename)\n",
    "    print(percentages)\n",
    "    knhdf5.close()\n",
    "    unhdf5.close()\n",
    "    return unknown_x, unknown_y, unknown_peaks, unknown_peak_assignments, percentages\n",
    "\n",
    "def compare_unknown_to_known(unknown_peaks, known_peaks, precision):\n",
    "    \"\"\"This function takes in peak positions for the spectrum to be\n",
    "    analyzed and a single known compound and determines if the peaks\n",
    "    found in the known compound are present in the unknown spectrum.\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(unknown_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `combined_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_peaks)))\n",
    "\n",
    "    if not isinstance(known_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_peaks)))\n",
    "# Need to check values within known_peaks\n",
    "\n",
    "    if not isinstance(precision, (float, int)):\n",
    "        raise TypeError(\"\"\"Passed value of `precision` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(precision)))\n",
    "\n",
    "    assignment_matrix = np.zeros(len(unknown_peaks))\n",
    "    peaks_found = 0\n",
    "    for i, _ in enumerate(unknown_peaks):\n",
    "        for j, _ in enumerate(known_peaks):\n",
    "            # instead of If, call peak_1D_score\n",
    "            if math.isclose(unknown_peaks[i], known_peaks[j],\n",
    "                            abs_tol=precision, rel_tol=1e-9):\n",
    "                # Instead of using a 1, just input the score\n",
    "                # from the score calculator.\n",
    "                # Bigger is better.\n",
    "                # Storing only the second component in the list.\n",
    "                assignment_matrix[i] = 1\n",
    "                peaks_found += 1\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "        if peaks_found == len(known_peaks):\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "    print(assignment_matrix)\n",
    "\n",
    "    return assignment_matrix\n",
    "\n",
    "def peak_position_comparisons(unknown_peaks, known_compound_peaks,\n",
    "                              association_matrix,\n",
    "                              knownhdf5_filename):\n",
    "    \"\"\"This function takes in an association matrix and turns the numbers\n",
    "    given by said matrix into a text label.\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(unknown_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `unknown_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_peaks)))\n",
    "    if not isinstance(known_compound_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_compound_peaks)))\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\"+ str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\"+ knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(association_matrix, list):\n",
    "        raise TypeError(\"\"\"Passed value of `association_matrix` is not a float\n",
    "        or int! Instead, it is: \"\"\" + str(type(association_matrix)))\n",
    "\n",
    "    # open .hdf5\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r+')\n",
    "\n",
    "    #Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\"+ str(type(known_compound_list)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list` is\n",
    "            not a string! Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "\n",
    "    unknown_peak_assignment = []\n",
    "    #Step through the unknown peaks to make an assignment for each unknown peak.\n",
    "\n",
    "    for i, _ in enumerate(unknown_peaks):\n",
    "        # We might be able to make a small performance\n",
    "        # improvement if we were to somehow\n",
    "        # not search the peaks we already had searched\n",
    "        # but that seems to not be trivial.\n",
    "        position_assignment = []\n",
    "        # We'll need an outer loop that walks through\n",
    "        # all the different compound positions\n",
    "        for j, _ in enumerate(known_compound_peaks):\n",
    "            if association_matrix[j][i] == 1:\n",
    "                position_assignment.append(known_compound_list[j])\n",
    "            else:\n",
    "                pass\n",
    "        if position_assignment == []:\n",
    "            position_assignment.append(\"Unassigned\")\n",
    "        else:\n",
    "            pass\n",
    "        unknown_peak_assignment.append(position_assignment)\n",
    "    knhdf5.close()\n",
    "    return unknown_peak_assignment\n",
    "\n",
    "\n",
    "def percentage_of_peaks_found(known_peaks, association_matrix, knownhdf5_filename):\n",
    "    \"\"\"This function takes in a list of classified peaks, and returns a percentage of\n",
    "    how many of the material's peaks are found in the unknown spectrum.\n",
    "    This can be used as a metric of confidence.\"\"\"\n",
    "\n",
    "    #Handle bad inputs\n",
    "    if not isinstance(known_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_peaks)))\n",
    "    if not isinstance(association_matrix, list):\n",
    "        raise TypeError(\"\"\"Passed value of `association_matrix` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(association_matrix)))\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\"+ str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "\n",
    "    # open .hdf5\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r+')\n",
    "\n",
    "    # Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\"+ str(type(known_compound_list)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list`\n",
    "            is not a string!\n",
    "            Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "    percentage_dict = {}\n",
    "    for j, _ in enumerate(known_compound_list):\n",
    "#         print(association_matrix)\n",
    "#         print(known_peaks)\n",
    "        count_number = sum(association_matrix[j])\n",
    "        percentage_dict[known_compound_list[j]] = float(count_number /\n",
    "                                                        (len(known_peaks[j]))) * 100\n",
    "    knhdf5.close()\n",
    "    return percentage_dict\n",
    "\n",
    "\n",
    "def plotting_peak_assignments(unknown_x, unknown_y, unknown_peaks,\n",
    "                              unknown_peak_assignments, unknownhdf5_filename,\n",
    "                              knownhdf5_filename,\n",
    "                              temp, time, peak_labels, exportlabelinput=True):\n",
    "    \"\"\"This function plots a set of unknown peaks, and plots the assigned\n",
    "    classification given by the functions within peakassignment\"\"\"\n",
    "\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(unknown_peaks, list):\n",
    "        raise TypeError(\"\"\"Passed value of `unknown_peaks` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_peaks)))\n",
    "\n",
    "    if not isinstance(unknown_x, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `unknown_x` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_x)))\n",
    "\n",
    "    if not isinstance(unknown_y, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\" Passed value of `unknown_y` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(unknown_y)))\n",
    "    # handling input errors\n",
    "    if not isinstance(unknownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `unknownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(unknownhdf5_filename)))\n",
    "    if not unknownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`unknownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + unknownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(knownhdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `knownhdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(knownhdf5_filename)))\n",
    "    if not knownhdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`knownhdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + knownhdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(temp, int):\n",
    "        raise TypeError(\"\"\"Passed value of `temp` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(temp)))\n",
    "    if not isinstance(time, int):\n",
    "        raise TypeError(\"\"\"Passed value of `time` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(time)))\n",
    "    if not isinstance(peak_labels, list):\n",
    "        raise TypeError(\"\"\"Passed value of `peak_labels` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(peak_labels)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(peak_labels):\n",
    "        if not isinstance(peak_labels[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `peak_labels` is not a string!\n",
    "            Instead, it is: \"\"\" + str(type(peak_labels[i])))\n",
    "    #Now we need to check the elements within the unknown_peak_assignment\n",
    "    #to make sure they are correct.\n",
    "    for i, _ in enumerate(unknown_peak_assignments):\n",
    "        if not isinstance(unknown_peak_assignments[i], list):\n",
    "            raise TypeError(\"\"\"Passed value within `unknown_peak_assignment`\n",
    "            is not a list!\n",
    "            Instead, it is: \"\"\" + str(type(unknown_peak_assignments[i])))\n",
    "            if not isinstance(unknown_peak_assignments[i][i], str):\n",
    "                raise TypeError(\"\"\"Passed value within `unknown_peak_assignment`\n",
    "                is not a string! \n",
    "                Instead, it is: \"\"\" + str(type(unknown_peak_assignments[i][i])))\n",
    "    # open .hdf5\n",
    "    knhdf5 = h5py.File(knownhdf5_filename, 'r')\n",
    "    unhdf5 = h5py.File(unknownhdf5_filename, 'r')\n",
    "    residuals = np.asarray(list(unhdf5['{}C/{}s/residuals'.format(temp, time)]))\n",
    "    #Extract keys from files\n",
    "    known_compound_list = list(knhdf5.keys())\n",
    "\n",
    "    if not isinstance(known_compound_list, list):\n",
    "        raise TypeError(\"\"\"Passed value of `known_compound_list` is not a list!\n",
    "        Instead, it is: \"\"\" + str(type(known_compound_list)))\n",
    "    # Now we need to check the elements within the known_compound_list\n",
    "    # to make sure they are correct.\n",
    "    for i, _ in enumerate(known_compound_list):\n",
    "        if not isinstance(known_compound_list[i], str):\n",
    "            raise TypeError(\"\"\"Passed value within `known_compound_list` is\n",
    "            not a string! Instead, it is: \"\"\" + str(type(known_compound_list[i])))\n",
    "    # extract spectra data\n",
    "    x_data = list(unhdf5['{}C/{}s/wavenumber'.format(temp, time)])\n",
    "    y_data = list(unhdf5['{}C/{}s/counts'.format(temp, time)])\n",
    "#     plt.plot(unknown_x, unknown_y, color='black', label='Unknown Spectrum')\n",
    "    if exportlabelinput:\n",
    "        print('export labelling only')\n",
    "    else:\n",
    "        peak_labels = []\n",
    "        for i, _ in enumerate(unknown_peak_assignments):\n",
    "            peak_labels.append(str(unknown_peak_assignments[i]))\n",
    "    print(peak_labels)\n",
    "    # plot spectra and peak labels\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True,\n",
    "                                   gridspec_kw={'height_ratios': [3, 1]},\n",
    "                                   figsize=(15, 6), dpi=300)\n",
    "    # plot data\n",
    "    ax1.plot(x_data, y_data, color='blue')\n",
    "    ax2.plot(x_data, residuals, color='teal')\n",
    "    lineid_plot.plot_line_ids(x_data, y_data, unknown_peaks,\n",
    "                              peak_labels, box_axes_space=0.30,\n",
    "                              plot_kwargs={'linewidth':1},\n",
    "                              max_iter=75, ax=ax1)\n",
    "#     fig.set_size_inches(15,5)\n",
    "    # lock the scale so that additional plots do not warp the labels\n",
    "    ax1.set_autoscale_on(False)\n",
    "    # Titles and labels\n",
    "    ax2.set_xlabel('Wavenumber ($cm^{-1}$)', fontsize=14)\n",
    "    ax1.set_xlim(min(x_data), max(x_data))\n",
    "    ax1.set_ylabel('Counts', fontsize=14, labelpad=20)\n",
    "    ax2.set_ylabel('Residuals', fontsize=14, labelpad=12)\n",
    "    # scale residuals plot symmetrically about zero\n",
    "    ylim = max(abs(min(residuals)), abs(max(residuals)))\n",
    "    ax2.set_ylim(-ylim, ylim)\n",
    "    # add grid lines to residual plot\n",
    "    ax2.grid(which='major', axis='y', linestyle='-')\n",
    "    # force tick labels for top plot\n",
    "    ax1.tick_params(axis='both', which='both', labelsize=10, labelbottom=True)\n",
    "    # add title\n",
    "    ax1.set_title('{}C/{}s spectra from {}'.format(temp,\n",
    "                                                   time,\n",
    "                                                   unknownhdf5_filename),\n",
    "                  fontsize=18, pad=350)\n",
    "    plt.show()\n",
    "    knhdf5.close()\n",
    "    unhdf5.close()\n",
    "\n",
    "def add_label(hdf5_filename, temp, time, peak, label):\n",
    "    \"\"\"Function that adds a label to a peak dataset in the hdf5 file\n",
    "    \"\"\"\n",
    "    #Handling errors in inputs.\n",
    "    if not isinstance(hdf5_filename, str):\n",
    "        raise TypeError(\"\"\"Passed value of `hdf5_filename` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(hdf5_filename)))\n",
    "    if not hdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError(\"\"\"`hdf5_filename` is not type = .hdf5!\n",
    "        Instead, it is: \"\"\" + hdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(temp, int):\n",
    "        raise TypeError(\"\"\"Passed value of `temp` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(temp)))\n",
    "    if not isinstance(time, int):\n",
    "        raise TypeError(\"\"\"Passed value of `time` is not a int!\n",
    "        Instead, it is: \"\"\" + str(type(time)))\n",
    "    if not isinstance(peak, str):\n",
    "        raise TypeError(\"\"\"Passed value of `peak` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(peak)))\n",
    "    if not isinstance(label, str):\n",
    "        raise TypeError(\"\"\"Passed value of `label` is not a string!\n",
    "        Instead, it is: \"\"\" + str(type(label)))\n",
    "    # open hdf5 file as read/write\n",
    "    hdf5 = h5py.File(hdf5_filename, 'r+')\n",
    "    # extract existing data from peak dataset\n",
    "    peak_data = list(hdf5['{}C/{}s/{}'.format(temp, time, peak)])[0]\n",
    "#     print(peak_data)\n",
    "    # make a new tuple that contains the orginal data as well as the label\n",
    "    label_tuple = (label,)\n",
    "    data = tuple(peak_data) +label_tuple\n",
    "    # delete the old dataset so the new one can be saved\n",
    "    del hdf5['{}C/{}s/{}'.format(temp, time, peak)]\n",
    "    # define a custom datatype that allows for a string as the the last tuple element\n",
    "    my_datatype = np.dtype([('fraction', np.float),\n",
    "                            ('center', np.float),\n",
    "                            ('sigma', np.float),\n",
    "                            ('amplitude', np.float),\n",
    "                            ('fwhm', np.float),\n",
    "                            ('height', np.float),\n",
    "                            ('area under the curve', np.float),\n",
    "                            ('label', h5py.special_dtype(vlen=str))])\n",
    "    # recreate the old dataset in the hdf5 file\n",
    "    dataset = hdf5.create_dataset('{}C/{}s/{}'.format(temp, time, peak),\n",
    "                                  (1,), dtype=my_datatype)\n",
    "    # apply custom dtype to data tuple\n",
    "#     print(dataset)\n",
    "    print(data)\n",
    "#     print(my_datatype)\n",
    "    data_array = np.array(data, dtype=my_datatype)\n",
    "    # write new values to the blank dataset\n",
    "    dataset[...] = data_array\n",
    "#     print(dataset)\n",
    "    hdf5.close()\n",
    "    return\n",
    "\n",
    "def peak_1d_score(row_i, row_j, scoremax, precision):\n",
    "    \"\"\"\n",
    "    Returns scores with respect to the repricoal of the\n",
    "    calculated Euclidean distance between peaks\n",
    "    #((x1-x2)^2) in 1D\n",
    "    #((x1-x2)^2 + (y1-y2)^2) in 2D\n",
    "\n",
    "    Parameters:\n",
    "        row_i (list like):  input list\n",
    "        row_j (list like): input list\n",
    "        scoremax (float): Euclidean reciprocal score divided by max score;\n",
    "        default is 1\n",
    "\n",
    "    Returns:\n",
    "        scores (list): Euclidean reciprocal scores\n",
    "        peaks (tuple): peaks associated with scores\n",
    "    \"\"\"\n",
    "    # Handling errors at the input\n",
    "    if not isinstance(row_i, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_i` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_i)))\n",
    "    if not isinstance(row_j, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_j` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_j)))\n",
    "    if not isinstance(scoremax, (float, int)):\n",
    "        raise TypeError(\"\"\"Passed value of `scoremax` is not a float or int!\n",
    "        Instead, it is: \"\"\" + str(type(scoremax)))\n",
    "    if scoremax < 0:\n",
    "        raise ValueError(\"\"\"Passed value of `scoremax` is not within bounds!\"\"\")\n",
    "\n",
    "    # Initializing the variables\n",
    "    scores = []\n",
    "    peaks = []\n",
    "\n",
    "    for i, _ in enumerate(row_i):\n",
    "        for j, _ in enumerate(row_j):\n",
    "            # Calculating distances between peaks\n",
    "            distance = np.where((row_i[i] - row_j[j] > precision), np.nan,\n",
    "                                math.sqrt(sum([math.pow(row_i[i] - row_j[j], 2)])))\n",
    "            # Score for peaks less than 50 units apart\n",
    "            if 1 / (distance + 1) > (1/precision):\n",
    "                # Dividing over the given max score\n",
    "                scores.append(((1 / (distance + 1)) / scoremax))\n",
    "                # Appends a tuple of the compared peaks\n",
    "                peaks.append((row_i[i], row_j[j]))\n",
    "            else:\n",
    "                pass\n",
    "    return scores, peaks\n",
    "\n",
    "\n",
    "def score_max(row_i, row_j, k, precision):\n",
    "    \"\"\"\n",
    "    Returns list of scores sorted with respect to the peaks\n",
    "    related to its output max score\n",
    "\n",
    "    Parameters:\n",
    "        row_i (list like):  input list\n",
    "        row_j (list like): input list\n",
    "        k (int): input integer used to sort the scores / kth highest score\n",
    "\n",
    "    Returns:\n",
    "        maxscores (list): Euclidean reciprocal score divided by max score\n",
    "        maxpeaks (tuple): peaks associated with max scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Handling errors at the input\n",
    "    if not isinstance(row_i, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_i` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_i)))\n",
    "    if not isinstance(row_j, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_j` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_j)))\n",
    "    if not isinstance(k, int):\n",
    "        raise TypeError(\"\"\"Passed value of `k` is not an int!\n",
    "        Instead, it is: \"\"\" + str(type(k)))\n",
    "    if k < 0:\n",
    "        raise ValueError(\"\"\"Passed value of `k` is not within bounds!\"\"\")\n",
    "    try:\n",
    "        scoremax = sorted(set(peak_1d_score(row_i, row_j, 1, precision)[0][:]))[-k]\n",
    "        maxscores, maxpeaks = peak_1d_score(row_i, row_j, scoremax, precision)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\"\"Function did not receive a scoremax variable. The variable\n",
    "        scoremax has been reset back to 1. This is equivalent to \n",
    "        your unnormalized score.\"\"\")\n",
    "\n",
    "        maxscores, maxpeaks = peak_1d_score(row_i, row_j, 1, precision)\n",
    "\n",
    "    return maxscores, maxpeaks\n",
    "\n",
    "\n",
    "def score_sort(row_i, row_j, k, precision):\n",
    "    \"\"\"\n",
    "    Returns list of scores sorted\n",
    "\n",
    "    Parameters:\n",
    "        list_input (list like):  input list\n",
    "        row (list like): input list\n",
    "        k (int): input integer used to sort the scores / kth highest score\n",
    "\n",
    "    Returns:\n",
    "        sortedscores (list): sorted Euclidean distances\n",
    "    \"\"\"\n",
    "    # Handling errors at the input\n",
    "    if not isinstance(row_i, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_i` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_i)))\n",
    "    if not isinstance(row_j, (list, np.ndarray)):\n",
    "        raise TypeError(\"\"\"Passed value of `row_j` is not a list or ndarray!\n",
    "        Instead, it is: \"\"\" + str(type(row_j)))\n",
    "    if not isinstance(k, int):\n",
    "        raise TypeError(\"\"\"Passed value of `k` is not an int!\n",
    "        Instead, it is: \"\"\" + str(type(k)))\n",
    "    if k < 0:\n",
    "        raise ValueError(\"\"\"Passed value of `k` is not within bounds!\"\"\")\n",
    "\n",
    "    sortedscores = []\n",
    "    sortedscores.append(score_max(row_i, row_j, k, precision))\n",
    "    sortedscores.sort()\n",
    "\n",
    "    return sortedscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the functions in the ^^above .py function (will probs be deleted for final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plotting_peak_assignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_peak_1d_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_peak_position_comparisons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage_of_peaks_found()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compare_unknown_to_known()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_peak_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_add_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datavis.plot_fit('peakidentify_experiment_test.hdf5', '300C/25s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_experiment_test.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of if the functions work/ minimum amount (deleted for final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('peakidentify_experiment_testnoadd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_experiment('peakidentify_experiment_testnoadd.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'peakidentify_calibration_file.hdf5'\n",
    "key = 'Methane'\n",
    "key2 = 'Hydrogen'\n",
    "key3 = 'CO2'\n",
    "hdf5_expfilename = 'peakidentify_experiment_testnoadd.hdf5'\n",
    "expkey = '300C/25s'\n",
    "temp = 300\n",
    "time = 25\n",
    "# open .hdf5\n",
    "calhdf5 = h5py.File(hdf5_calfilename, 'r+')\n",
    "exphdf5 = h5py.File(hdf5_expfilename, 'r+')\n",
    "# extract spectra data\n",
    "known_x = list(calhdf5['{}/wavenumber'.format(key)])\n",
    "known_y = list(calhdf5['{}/counts'.format(key)])\n",
    "residuals = np.asarray(list(exphdf5['{}/{}/residuals'.format(str(temp)+'C', str(time)+'s')]))\n",
    "unknown_x = list(exphdf5['{}/{}/wavenumber'.format(str(temp)+'C', str(time)+'s')])\n",
    "unknown_y = list(exphdf5['{}/{}/counts'.format(str(temp)+'C', str(time)+'s')])\n",
    "# extract fitted peak center values\n",
    "M_peaks = []\n",
    "H_peaks = []\n",
    "CO2_peaks = []\n",
    "for _,peak in enumerate(list(calhdf5[key])[:-3]):\n",
    "#     print(peak)\n",
    "#     print(list(calhdf5['{}/{}'.format(key, peak)])[0][2])\n",
    "    M_peaks.append(list(calhdf5['{}/{}'.format(key, peak)])[0][2])\n",
    "unknown_peakstest = []\n",
    "for _,peak in enumerate(list(calhdf5[key2])[:-3]):\n",
    "    H_peaks.append(list(calhdf5['{}/{}'.format(key2, peak)])[0][2])\n",
    "for _,peak in enumerate(list(calhdf5[key3])[:-3]):\n",
    "    CO2_peaks.append(list(calhdf5['{}/{}'.format(key3, peak)])[0][2])\n",
    "for i,peak in enumerate(list(exphdf5[expkey])[:-3]):\n",
    "    if i < 9:\n",
    "        unknown_peakstest.append(list(exphdf5['{}/{}/Peak_0{}'.format(str(temp)+'C', str(time)+'s', i+1)])[0][2])\n",
    "    else:\n",
    "        unknown_peakstest.append(list(exphdf5['{}/{}/Peak_{}'.format(str(temp)+'C', str(time)+'s', i+1)])[0][2])\n",
    "known_x = np.asarray(known_x)\n",
    "known_y = np.asarray(known_y)\n",
    "unknown_x = np.asarray(unknown_x)\n",
    "unknown_y = np.asarray(unknown_y)\n",
    "known_compound_list = list(calhdf5.keys())\n",
    "precision = 50\n",
    "known_peaks_listtest = [H_peaks, M_peaks, CO2_peaks]\n",
    "known_peakstest = []\n",
    "association_matrixtest = []\n",
    "# for i, _ in enumerate(known_compound_list):\n",
    "#     for _,peak in enumerate(list(hdf5[key])[:-3]):\n",
    "#         known_peakstest.append(known_peaks_listtest[i])\n",
    "#         #print(type(known_peaks))\n",
    "#         association_matrixtest.append(compare_unknown_to_known(\n",
    "#             unknown_peakstest, known_peakstest[i], precision,\n",
    "#             hdf5_expfilename, expkey))\n",
    "\n",
    "        \n",
    "#OK, next identify all of the peaks present in the known compound set.\n",
    "    #For efficiency, we'll also compare them against the unknown in the same for loop.\n",
    "known_peaks = []\n",
    "known_peaks_list = []\n",
    "num_peaks_list = []\n",
    "assignment_matrix = []\n",
    "split__index_list = []\n",
    "for i, _ in enumerate(known_compound_list):\n",
    "    print(\"The peaks that we found for \"\n",
    "      + str(known_compound_list[i]) + \" are: \")\n",
    "    num_peaks_list.append(len(list(calhdf5[known_compound_list[i]])[:-3]))\n",
    "    split__index_list.append(sum(num_peaks_list))\n",
    "    for j,peak in enumerate(list(calhdf5[known_compound_list[i]])[:-3]):\n",
    "        print(list(calhdf5['{}/{}'.format(known_compound_list[i], peak)])[0][2])\n",
    "        # Need to separate known peaks to make a list of two separate lists\n",
    "        # to perform custom list split using list comprehension + zip() and split_index_list\n",
    "        known_peaks_list.append(list(calhdf5['{}/{}'.format(known_compound_list[i], peak)])[0][2])\n",
    "        result = [known_peaks_list[i : j] for i, j in zip([0] + split__index_list, split__index_list + [None])] \n",
    "    known_peakstest.append(result)\n",
    "    association_matrixtest.append(peakidentify.compare_unknown_to_known(\n",
    "        unknown_peakstest, known_peakstest[i][i], precision))        \n",
    "       \n",
    "\n",
    "unknown_peak_assignmentstest = peakidentify.peak_position_comparisons(\n",
    "    unknown_peakstest,\n",
    "    known_peakstest,\n",
    "    association_matrixtest,\n",
    "    hdf5_calfilename)\n",
    "peak_labels=[]\n",
    "for i, _ in enumerate(unknown_peak_assignmentstest):  \n",
    "        peak_labels.append(str(unknown_peak_assignmentstest[i]))\n",
    "peakidentify.plotting_peak_assignments(unknown_x,\n",
    "                                       unknown_y,\n",
    "                                       unknown_peakstest,\n",
    "                                       unknown_peak_assignmentstest,\n",
    "                                       hdf5_expfilename,\n",
    "                                       hdf5_calfilename, temp, time, peak_labels, exportlabelinput = False )\n",
    "percentages = peakidentify.percentage_of_peaks_found(known_peakstest[len(known_compound_list)-1],\n",
    "                                            association_matrixtest,\n",
    "                                            hdf5_calfilename)\n",
    "print(percentages)\n",
    "calhdf5.close()\n",
    "exphdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_experiment_testnoadd.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Wrapper Function - peak_assignment\n",
    "\n",
    "`peak_assignment` takes as parameters five arguments, of which 2 are optional. First, we must pass the hdf5 filename from the experiment hdf5 file, which is the `hdf5_expfilename`. The second parameter is the key from the experiment hdf5 file, which is the `expkey`. The third parameter is the the hdf5 filename from the calibration hdf5 file, which is the `hdf5_filename`. This calibration hdf5 file contains a list of all of the compounds that you want to check your unknown data for. In other words, if a compound data is not passed in this list, then the function will not look to see if that compound is present in your unknown spectrum. Each of these compounds can be refered to by the key function of the hdf5 file. The fourth parameter contains a list of strings all of the compounds that that are in that calibration file mentioned before.  This is the list that we generated above titled `list_of_compounds`, which we passed data for carbon dioxide, hydrogen, methane. An additional note is that the list of compounds from the calibration file will be listed alphabetically due to the functions of the hdf5 file format. Matching this alphabetical order is important when referencing the `list_of_compounds`. Parameter 5 is `precision`, which is an optional parameter. It is defaulted to 0.08, but can be adjusted by passing a different float value. What it does is it sets a boundary for how close the known peak position has to be to an unknown peak for the function to assign the peak. It is a percentage value, so 0.08 is 8 % of the larger of the two peak positions. The default of .08 will encompass all of the peaks in this example, but adjustments are recommended for your own experimental/calibration data. Finally, parameter 6 is `plot`, which is also an optional parameter. It is a Boolean which defaults to True. If True, this will plot the unknown specrta and labels for all of the peaks in said spectra. \n",
    "\n",
    "For our example, we won't mess with those optional parameters.\n",
    "\n",
    "This function will output the list of known peaks for each compound and the plot for the unknown specrta and labels for all of the peaks in said spectra. Additionally, it will output a percentage of how many of the material's peaks are found in the unknown spectrum. This can be used as a metric of confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'peakidentify_calibration_file.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'peakidentify_experiment_file.hdf5'\n",
    "temp = 300\n",
    "time = 25\n",
    "unknown_x, unknown_y, unknown_peaks, unknown_peak_assignments, assignment_matrix, temp, time, percentages = peakidentify.peak_assignment(hdf5_expfilename, temp, time, hdf5_calfilename, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '300C/25s'\n",
    "dataprep.view_hdf5('peakidentify_experiment_file.hdf5')\n",
    "fig, ax1, ax2 = datavis.plot_fit(hdf5_expfilename, key)\n",
    "datavis.plot_components(ax1, hdf5_filename, key, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Because plot = True, we see a plot of our unknown spectra with lines indicating the positions of identified possible components. \n",
    "\n",
    "To provide additional clarity as to the likelihood of which peaks are correctly assigned, and which are not, a dictionary is printed out after showing the plot. The dictionary contains the name of the compound, followed by a percentage. The percentage is generated by comparing the total number of peaks in the Raman spectra that are found in the unknown spectra. For example, if 1 peak out of 10 possible are found as is the case for carbon dioxide above, it will show a percentage of 10%, whereas for water if it find 3 peaks out of a possible 3, the percentage reported is 100%. This percentage provides an approximate degree of confidence for how likely it is that a compound is present. \n",
    "\n",
    "For this above example, because we see that water and carbon monoxide show a 100% detection percentage, we're quite confident that they are in-fact present in our unknown sample; however, as carbon dioxide only shows a 10% detection percentage, it's not very likely that it is present in our unknown sample. That said, the final value judgement is left to the user, as these percentages are just approximate guidelines. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Individual Functions of `peakidentify.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen the broad usage of peakidentify.py, we'll walk through the individual functions and spend a bit more time really explaining each individual use-case.\n",
    "\n",
    "You have already been introduced to the first function, a wrapper function called peak_assignment. Following this, the next function is compare_unknown_to_known. This function takes in the peak positions (the wavenumber position for the peak position) for the unknown's spectrum, and compares it to the peaks positions for a single known compound. \n",
    "\n",
    "compare_unknown_to_known takes 3 parameters. The first parameter, combined_peaks is the peak positions of the unknown peaks. The second parameter, known_peaks, is the peak positions of one of the known compounds. The third parameter is precision, which is the maximum percentage difference between an unknown peak position and a known peak position that will be accepted as close enough to give a positive assignment. The function returns a parameter association_matrix, which contains a list of either 1 or 0, which is the assignment value of whether or not a peak from the unknown peak set was found in the passed known peak set. \n",
    "\n",
    "We'll need to generate some data to pass along, then call the function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.compare_unknown_to_known(unknown_peaks, H_peaks, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.compare_unknown_to_known(unknown_peaks M_peaks, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.compare_unknown_to_known(unknown_peaks, CO2_peaks, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this function, when called to compare water to the unknown peaks, or carbon monoxide to the unknown peaks, we see that it returns an array of 1 and 0. This array is an assignment array. If the array contains a 0, the peak in that position is not found in the known compound. For example, for the water peak comparison, the value of the returned array is 1 at position 0. That means that it is assigning the peak found at position 0 to be possibly coming from water. The 0 in position 2 implies that the unknown peak is not found in water. If we look at position 2 in the carbon monoxide comparison, we see that it is a 1 there. This implies that the peak in position 2, is likely not coming from water, and could possibly be coming from carbon monoxide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function to investigate is the `peak_position_comparisons`. It takes four parameters. The first is the list of unknown peaks, and the second is the complete list of peaks for known compounds. The third is the list of compounds that the wrapper function should search through. The fourth is the complete association matrix output from the `compare_unknown_to_known` function. By appending together the matrices returned by that function, the full association_matrix is built. We will build a full association matrix, and then pass it to our `peak_position_comparisons` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our test data.\n",
    "association_matrix_beta = []\n",
    "known_peaks_beta = [CO2_peaks,H_peaks, M_peaks]\n",
    "# Note the order that you append to the association_matrix_beta matters \n",
    "# with the known compound list so make sure these match alphabetically\n",
    "# Known peaks beta, order does not matter\n",
    "association_matrix_beta.append(peakidentify.compare_unknown_to_known(unknown_peaks, CO2_peaks, 50))\n",
    "association_matrix_beta.append(peakidentify.compare_unknown_to_known(unknown_peaks, H_peaks, 50))\n",
    "association_matrix_beta.append(peakidentify.compare_unknown_to_known(unknown_peaks, M_peaks, 50))\n",
    "print(association_matrix_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_compound_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_peakstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peakidentify.peak_position_comparisons(unknown_peaks, known_peaks_beta[len(known_compound_list)-1], association_matrix_beta, hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list that shows all of the possible assignments for every peak, in a text format. We can see that for the unknown peak in position 0, there is a potential for that peak to have come from Hydrogen or Methane, whereas the peak in position 2, is labelled as Hydrogen. If this function does not find any possible compound for a peak, it returns a value of 'unassigned', which we will show below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.peak_position_comparisons(unknown_peaks, [M_peaks], [[1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peakidentify.peak_position_comparisons(unknown_peaks, [H_peaks],[[0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_position_comparisons(unknown_peaks, [CO2_peaks], [[1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if the peak isn't found, it labels the value as 'unassigned'! \n",
    "\n",
    "Additionally, the association matrix list of 1s and 0s directly correlates to the list of possible assignments, thus changes in the matrix will affect the possible assignments.\n",
    "\n",
    "\n",
    "\n",
    "The next function to be explored is `percentage_of_peaks_found`. This function calculates a percentage value for how many of the peaks from a known compound were found in the unknown peak list. This function takes 3 parameters. The first is a list of known peaks, the second is the full association matrix, and the third is the list of known compounds, as a list of dictionaries. The function returns a dictionary where the keys are all of the titles of the known compounds, and the values of the keys are all of the percentages of found peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pecentage of peaks found case\n",
    "# Adding random lists not associated with the known compound list will throw off the calculations of found peak percent\n",
    "peakidentify.percentage_of_peaks_found([CO2_peaks, [1], H_peaks, M_peaks ], \n",
    "                          association_matrix_beta, hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THe known peaks found must match the order from the known_compound_list\n",
    "peakidentify.percentage_of_peaks_found([CO2_peaks,H_peaks, M_peaks], \n",
    "                          association_matrix_beta, hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.percentage_of_peaks_found(known_peaks_beta, \n",
    "                          association_matrix_beta, hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.percentage_of_peaks_found([CO2_peaks,[0], [0]], \n",
    "                          association_matrix_beta, hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, above you can see that 100% of the peaks of water are found, as are 100% of the carbon monoxide peaks. This is pretty reasonable, as the unknown spectra was made up of water and carbon monoxide, and it's a good sign. There is a 10% find-rate for carbon dioxide. If we look at the absolute peak positions for carbon dioxide, we see that this is pretty reasonable, as one of the peaks of carbon dioxide closely overlaps with the first peak of water. However, none of the other carbon dioxide peaks overlap, so overall we get a pretty low percentage of carbon dioxide peaks found. Ultimately, it is up to the user to be sure that the percentages and compound list make sense, and to interpret what the percentages mean.\n",
    "\n",
    "The next function in the peakidentify system is the `plotting_peak_assignments` function, which plot the full unknown spectra, and labels each of the peaks in the unknown spectra with all of the possible compounds present. It takes as inputs 4 parameters. The first is the unknown_x data from before, and the second is the unknown_y data, also from before. The third input is the list of unknown peaks, and the fourth input is the list of assignments for each of those unknown peaks. It does not return any values, but it does generate a plot of all of the input information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unknown_peak_assignments_beta = peakidentify.peak_position_comparisons(unknown_peakstest, known_peaks_beta, association_matrix_beta, hdf5_calfilename)\n",
    "print(unknown_peak_assignments_beta)\n",
    "peakidentify.plotting_peak_assignments(unknown_x, unknown_y, unknown_peaks, unknown_peak_assignments_beta, hdf5_expfilename, hdf5_calfilename, temp, time, peak_labels, exportlabelinput = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the exact same plot as above, when we just interacted with the wrapper function. Ultimately, the plot generated should be exactly the same, given that it was generated using the same data. \n",
    "You can change the inputs for the known peaks, but you need to make sure your list of compounds is alphabetical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Demonstration of Two Peaks on top of each other\n",
    "(A) For each of vertical line, you can see that the peak labelling function has assigned not just one, but two possible peaks to this position. This is possible with chemicals that have common degradation derivatives.\n",
    "\n",
    "In the demo below of two peaks sharing one spot, the association matrix has been modified to add CO2 and Hydrogen peaks together in the 2nd peak placement, therefore the label shows CO2&Hydrogen.\n",
    "\n",
    "(B) In the future, there might be instances with two peaks peaks that exist in similar locations within the `precision` error bound. \n",
    "\n",
    "To provide additional clarity as to the likelihood of which peaks are correctly assigned, and which are not, a dictionary is printed out after showing the plot. The dictionary contains the name of the compound, followed by a percentage. The percentage is generated by comparing the total number of peaks in the Raman spectra that are found in the unknown spectra. For example, if 1 peak out of 10 possible are found as is the case for carbon dioxide above, it will show a percentage of 10%, whereas for water if it find 3 peaks out of a possible 3, the percentage reported is 100%. This percentage provides an approximate degree of confidence for how likely it is that a compound is present. \n",
    "\n",
    "For this above example, because we see that water and carbon monoxide show a 100% detection percentage, we're quite confident that they are in-fact present in our unknown sample; however, as carbon dioxide only shows a 10% detection percentage, it's not very likely that it is present in our unknown sample. That said, the final value judgement is left to the user, as these percentages are just approximate guidelines. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our test data.\n",
    "known_peaks_beta = [CO2_peaks,H_peaks, M_peaks]\n",
    "# Note the order that you append to the association_matrix_beta matters \n",
    "# with the known compound list so make sure these match alphabetically\n",
    "# Known peaks beta, order does not matter\n",
    "association_matrix_gamma = [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.], [1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]]\n",
    "print(association_matrix_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_peak_assignments_gamma = peakidentify.peak_position_comparisons(unknown_peakstest, known_peaks_beta, association_matrix_gamma, hdf5_calfilename)\n",
    "print(unknown_peak_assignments_gamma)\n",
    "peakidentify.plotting_peak_assignments(unknown_x, unknown_y, unknown_peakstest, unknown_peak_assignments_gamma, hdf5_expfilename, hdf5_calfilename, temp, time, peak_labels, exportlabelinput = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Peakidentify adjusting peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you add a new hdf5 file you use this function below\n",
    "dataprep.new_hdf5('peakidentify_adjust_peaks')\n",
    "# dataprep.add_experiment('peak_assignment_test.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataprep.view_hdf5('peak_assignment_test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename = 'peakidentify_adjust_peaks.hdf5'\n",
    "key = '300C/25s'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "# dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_35s.csv')\n",
    "# dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_45s.csv')\n",
    "# dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_55s.csv')\n",
    "# dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_65s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_list = [1270, 1350, 1385]#, 1640]\n",
    "# add_list = None\n",
    "drop_list = ['Peak_01']#, 'Peak_02']\n",
    "# drop_list = None\n",
    "\n",
    "dataprep.adjust_peaks(hdf5_filename, key, add_list, drop_list, plot_fits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5(hdf5_filename)\n",
    "fig, ax1, ax2 = datavis.plot_fit(hdf5_filename, key)\n",
    "datavis.plot_components(ax1, hdf5_filename, key, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5 = h5py.File(hdf5_filename, 'r+')\n",
    "# del hdf5['300C/35s']\n",
    "# hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'peakidentify_calibration_file.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'peakidentify_adjust_peaks.hdf5'\n",
    "temp = 300\n",
    "time = 25\n",
    "unknown_x, unknown_y, unknown_peaks, unknown_peak_assignments, assignment_matrix, temp, time, percentages = peakidentify.peak_assignment(hdf5_expfilename, temp, time, hdf5_calfilename, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_adjust_peaks.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_adjust_peaks.hdf5')\n",
    "fig, ax1, ax2 = datavis.plot_fit(hdf5_filename, key)\n",
    "datavis.plot_components(ax1, hdf5_filename, key, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5 Score Sort for 1D Wavenumber\n",
    "This is a metric of confidence. This score function should be considered as a first visual grading that your peaks are close enough to determine matching. We will provide a simplistic 1D Euclidean distance calculation. \n",
    "\n",
    "The reciprocal of the distance is returned as a percentage of distance comparison in the x dimension with 1 equalling peak centers that are super close and 0 super far away respectively. A tuple of the two peak locations compared is returned for visual confirmation of the score.\n",
    "\n",
    "This scoring function also includes the user input for the choice of normalizing the peak distance scores and sorting them based on the  kth highest score in the peak set. This normalization is performed to bring a better understanding to the peak distance confidence score, and it also allows for future work to be done in determining the best kth highest score normalization to be used. The default k for `peak_1d_score` is zero and is simlpy an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakidentify.peak_1d_score(unknown_peakstest,H_peaks,scoremax =1, precision = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score_max` function calls upon the `peak_1D_score` to get the highest score = 1 and then will normalize the rest of the score data respectively. Here, the user can input different k values for the # k highest score to be normalized over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoremaxk1=peakidentify.score_max(unknown_peakstest,H_peaks,k=1, precision = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score_sort` function just appends the score max normalized scores and peak tuples to a list. There is no additional sorting feature added since the score max function already sorts the scores from the lowest to highest peak tuples compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=peakidentify.score_sort(unknown_peakstest,H_peaks,k=1, precision = 50)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user should spend some time evaluating how to visually grade their peak distance closeness. It might make sense in some cases to not have a normalized highest peak score. This last example showcases how different k values interact with the score grading. The final k value will output the original not normalized score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_range = range(1,len(unknown_peakstest))\n",
    "precision = 50\n",
    "for k in k_range:\n",
    "    compdf = pd.DataFrame(data=peakidentify.score_sort(unknown_peakstest,H_peaks,k, precision)[0][0][:],columns=['unknown_vs_H_peak_Scores'])\n",
    "    compdf=compdf.assign(unknown_vs_H_peaks=peakidentify.score_sort(unknown_peakstest,H_peaks,k, precision)[0][1][:])\n",
    "    print('This score is normalized over the #'+str(k) + ' highest score in the peak set')\n",
    "    print(compdf)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    compdf2=pd.DataFrame(data=peakidentify.score_sort(unknown_peakstest,M_peaks,k, precision)[0][0][:],columns=['unknown_vs_M_peak_Scores'])\n",
    "    compdf2=compdf2.assign(unknown_vs_M_peaks=peakidentify.score_sort(unknown_peakstest,M_peaks,1, precision)[0][1][:])\n",
    "    print('This score is normalized over the #'+str(k) + ' highest score in the peak set')\n",
    "    print(compdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    compdf3=pd.DataFrame(data=peakidentify.score_sort(unknown_peakstest,CO2_peaks,k, precision)[0][0][:],columns=['unknown_vs_CO2_peak_Scores'])\n",
    "    compdf3=compdf3.assign(unknown_vs_M_peaks=peakidentify.score_sort(unknown_peakstest,CO2_peaks,1, precision)[0][1][:])\n",
    "    print('This score is normalized over the #'+str(k) + ' highest score in the peak set')\n",
    "    print(compdf3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('peakidentify_experiment_file.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the hdf5 files to keep the system data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('peakidentify_calibration_file.hdf5')\n",
    "os.remove('peakidentify_experiment_file.hdf5')\n",
    "os.remove('peakidentify_add_label_test.hdf5')\n",
    "os.remove('peakidentify_label_test.hdf5')\n",
    "os.remove('peakidentify_experiment_test.hdf5')\n",
    "os.remove('peakidentify_experiment_testnoadd.hdf5')\n",
    "os.remove('peakidentify_adjust_peaks.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
